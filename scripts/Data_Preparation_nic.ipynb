{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import codecs, json \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the correlation matrix considering al the inputs and all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "data_path = \"../dataset/train.csv\"\n",
    "y_loaded, data_loaded, _ = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "data_path = \"../dataset/test.csv\"\n",
    "_, data_test_loaded, _ = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 30), (568238, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaded.shape, data_test_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged= np.concatenate((data_loaded,data_test_loaded), axis=0)\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN DATA: YOU MAY WANT TO CHAGE THIS STEP\n",
    "data = drop_columns_with_70_nan_ratio(data_merged)\n",
    "data = fill_with_nan_list(data, nan_values=[0, -999])\n",
    "data = sustitute_nans(data, substitutions=np.nanmean(data, axis=0))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE AND STORE CORRELATION MATRIX\n",
    "corr_matrix = np.abs(np.corrcoef(data.T)) \n",
    "file_path = \"corr_matrix23.json\"\n",
    "json.dump(corr_matrix.tolist(), codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: LOAD CORRELATION MATRIX\n",
    "file_path = \"corr_matrix23.json\"\n",
    "obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "b_new = json.loads(obj_text)\n",
    "corr_matrix_loaded = np.array(b_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exaple: GET CORRELATED COLUMNS\n",
    "corr_matrix = np.abs(corr_matrix) > 0.6\n",
    "# remove i from the list (i is surely correlated to himself)\n",
    "for i in range(23):\n",
    "    corr_matrix[i][i] = False\n",
    "    \n",
    "correlated_to = {}\n",
    "for i in range(23):\n",
    "    c = np.where(corr_matrix[i])[0]\n",
    "    if len(c) > 0: # if it is not correlated to any other column then ignore it\n",
    "        correlated_to[i] = c\n",
    "correlated_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ditributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data as it is\n",
    "plot_distributions(data_loaded, y_loaded, col_labels = column_labels(), title = \"distribution_rawData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data after cleaning it is\n",
    "x_cleaned, keptCols = clean_x(data_loaded, 0.7, subs_func = np.nanmean)\n",
    "\n",
    "plot_distributions(x_cleaned, y_loaded, col_labels = keptCols, title = \"distribution_cleanedData_999=mean_0=mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data after cleaning it is\n",
    "x_cleaned, keptCols = clean_x(data_loaded, 0.7, subs_func = None)\n",
    "\n",
    "plot_distributions(x_cleaned, y_loaded, col_labels = keptCols, title = \"distribution_cleanedData_999=999_0=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better norm the hist to have a clearer idea of the proportions\n",
    "f, ax = plt.subplots(2, 2)\n",
    "dataset1 = [0, 0, 0, 0, 1, 2, 4, 6]\n",
    "dataset2 = [0, 0, 1, 6]\n",
    "\n",
    "ax[0][0].hist(dataset1, histtype='step',color=\"red\", normed=True)\n",
    "ax[0][1].hist(dataset1, histtype='step',color=\"red\", normed=False)\n",
    "\n",
    "\n",
    "ax[1][0].hist(dataset2, histtype='step', color=\"blue\", normed=True)\n",
    "ax[1][1].hist(dataset2, histtype='step', color=\"blue\", normed=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = fill_with_nan_list(data_merged, nan_values=[0, -999])\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 101, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 30)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]\n",
    "scan_perc = list(range(0, 101, 5))\n",
    "col_perc = np.zeros((len(scan_perc), data_merged.shape[1]))\n",
    "for col in range(data_merged.shape[1]):\n",
    "    for i, p in enumerate(scan_perc):\n",
    "         col_perc[i, col] = np.nanpercentile(data_merged[:, col], p)\n",
    "col_perc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store both the computed percentiles and the list of % to which they correspond\n",
    "file_path = \"percentiles.json\"\n",
    "map_ = {\n",
    "    \"scan_perc\": scan_perc,\n",
    "    \"col_perc\": col_perc.tolist()\n",
    "}\n",
    "json.dump(map_, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approach: split data depending on jet number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why divide data depending on jet numbers and which columns can we drop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: most of the -999 value are associated with jet number <= 1. The only feature that still contains some -999 values is the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data depending on the jet number (column 22) that is a categorical number in {0, 1, 2, 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((327371, 30), (252882, 30), (165027, 30), (72958, 30))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jets_0 = all_data[all_data[:, 22]==0, :]\n",
    "jets_1 = all_data[all_data[:, 22]==1, :]\n",
    "jets_2 = all_data[all_data[:, 22]==2, :]\n",
    "jets_3 = all_data[all_data[:, 22]==3, :]\n",
    "jets_0.shape, jets_1.shape, jets_2.shape, jets_3.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the -999 values?\n",
    "- jet = 0: columns [4, 5, 6, 12, 23, 24, 25, 26, 27, 28] contain only -999 values, 26.1% of entries in the first column have -999\n",
    "- jet = 1: columns [4, 5, 6, 12, 26, 27, 28] contain only -999 values, 7.6% of entries in the first column have -999\n",
    "- jet = 2: 3% of entries in the first column have -999\n",
    "- jet = 3: 1.4% of entries in the first column have -999\n",
    "\n",
    "The two tables relative to jet 2 and jet 3 could be merged. The -999 values in the first column will be removed from that column and a boolean column will be created to indicate the position of the -999 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet, cur_set in enumerate([jets_0, jets_1, jets_2, jets_3]):\n",
    "    print(\"Features in the dataset with jet=\", jet, \"contains this many values != -999\")\n",
    "    for col in range(30):\n",
    "        print(col, np.sum(cur_set[:, col] != -999))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the 0 values?\n",
    "- jet = 0: column 29 contains only 0s and, obviously, column 22 too since it stores the jet num.\n",
    "- jet = 1: spread\n",
    "- jet = 2: spread\n",
    "- jet = 3: spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet, cur_set in enumerate([jets_0, jets_1, jets_2, jets_3]):\n",
    "    print(\"Features in the dataset with jet=\", jet, \"contains this many values != 0\")\n",
    "    for col in range(30):\n",
    "        print(col, np.sum(cur_set[:, col] != 0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this first step we surely want to drop the following columns since they do not contain any useful information:\n",
    "- jet = 0: [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "- jet = 1: [4, 5, 6, 12, 22, 26, 27, 28] \n",
    "- jet = 2: [22]\n",
    "- jet = 3: [22]\n",
    "\n",
    "The column 22 is dropped in every obtained dataset since it just stores a constant representing the jet number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data and compute the correlation matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now divide our data, drop the above columns and verify if there are some highly correlated features. If so, it is worth trying to drop all but 1 column in a set of correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((327371, 18), (252882, 22), (165027, 29), (72958, 29))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data\n",
    "datasets = split_input_data(data_merged) # split and drop\n",
    "datasets[0].shape, datasets[1].shape, datasets[2].shape, datasets[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18, 18), (22, 22), (29, 29), (29, 29))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute correlation matrices\n",
    "corr_matrices = [None]*4\n",
    "for jet in range(4):\n",
    "    # don't consider the first column since it contains nan values (we will simply keep that column)\n",
    "    corr_matrices[jet] = np.corrcoef(datasets[jet][:, 1:].T) \n",
    "    \n",
    "    # to keep the same indexing of the columns just add one row above and one column at the left\n",
    "    corr_matrices[jet] = np.column_stack((np.zeros((corr_matrices[jet].shape[0], 1)), corr_matrices[jet]))   \n",
    "    corr_matrices[jet] = np.row_stack((np.zeros((1, corr_matrices[jet].shape[1])), corr_matrices[jet]))\n",
    "\n",
    "corr_matrices[0].shape, corr_matrices[1].shape, corr_matrices[2].shape, corr_matrices[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.row_stack((np.array([1, 2, 3]), np.zeros((3, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "21\n",
      "28\n",
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18, 18, 18, 18)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the mapping of correlations > min_corr\n",
    "min_corr = 0.7\n",
    "corr_mappings = [{}]*4\n",
    "for jet in range(4):\n",
    "    corr_matrix_bool = np.abs(corr_matrices[jet]) > min_corr \n",
    "    nfeature = corr_matrix_bool.shape[0]\n",
    "    # i is surely correlated to itself, drop that (useless) information\n",
    "    for i in range(nfeature):\n",
    "        corr_matrix_bool[i][i] = False\n",
    "\n",
    "    # compute the mapping of correlations\n",
    "    for i in range(nfeature):\n",
    "        c = np.where(corr_matrix_bool[i])[0].tolist()\n",
    "        if len(c) > 0: # if it is not correlated to any other column then ignore it\n",
    "            corr_mappings[jet][i] = c\n",
    "            \n",
    "len(corr_mappings[0]), len(corr_mappings[1]), len(corr_mappings[2]), len(corr_mappings[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr_mappings[0].copy()\n",
    "tobe_deleted = []\n",
    "# fetch all the columns that can be deleted and put them in tobe_deleted\n",
    "for _ in range(len(corr)): \n",
    "    longer_key = -1 \n",
    "    longer_length = 0\n",
    "\n",
    "    # look for the longer list\n",
    "    for key in corr:\n",
    "        curr_length = len(corr[key])\n",
    "        if curr_length > longer_length:\n",
    "            longer_length = curr_length\n",
    "            longer_key = key\n",
    "\n",
    "    if longer_length == 0: # the map is now empty\n",
    "        break\n",
    "        \n",
    "    tobe_deleted.append(corr[longer_key])\n",
    "    # delete all the columns that are correlated to column longer_key\n",
    "    # i.e. all the column whose index is in  corr[longer_key]\n",
    "    for corr_colum in corr[longer_key]:\n",
    "        corr[corr_colum] = []\n",
    "\n",
    "    # since those columns have been dropped they must be removed from all the other lists\n",
    "    for key in corr: \n",
    "        if key != longer_key:\n",
    "            #print(key, corr[key], \"-\", corr[longer_key], \"=\", list(set(corr[key]) - set(corr[longer_key])))\n",
    "            corr[key] = list(set(corr[key]) - set(corr[longer_key]))\n",
    "    corr[longer_key] = []\n",
    "\n",
    "tobe_deleted = [val for sublist in tobe_deleted for val in sublist]\n",
    "tobe_deleted.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [14],\n",
       " 2: [18],\n",
       " 3: [4, 5],\n",
       " 4: [3],\n",
       " 5: [3],\n",
       " 6: [11],\n",
       " 8: [20, 21, 24, 27],\n",
       " 9: [15],\n",
       " 11: [6],\n",
       " 14: [0],\n",
       " 15: [9],\n",
       " 16: [2, 5, 17, 20],\n",
       " 17: [2, 5, 16, 20],\n",
       " 18: [2],\n",
       " 20: [8, 21, 24, 27],\n",
       " 21: [8, 20, 27],\n",
       " 24: [8, 20, 27],\n",
       " 27: [8, 20, 21, 24]}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mappings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 6, 14, 15, 17, 20, 21, 24, 27]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tobe_deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must use the same standardisation process both for the training and for predicting. Since the -999 values will be dropped from the first column I remove them to compute the mean and the std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((327371, 18), (252882, 22), (165027, 29), (72958, 29))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = split_input_data(data_merged)\n",
    "datasets[0].shape, datasets[1].shape, datasets[2].shape, datasets[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_jet0 = np.corrcoef(datasets[0][:, 1:].T)\n",
    "corr_jet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "d = datasets[0]\n",
    "for col in range(d.shape[1]):\n",
    "    print(np.all(d[:, col]==d[:, col][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
