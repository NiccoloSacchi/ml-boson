{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import codecs, json \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and merge training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../dataset/train.csv\"\n",
    "y_loaded, data_loaded, _ = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../dataset/test.csv\"\n",
    "_, data_test_loaded, _ = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 30), (568238, 30))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaded.shape, data_test_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = np.concatenate((data_loaded,data_test_loaded), axis=0)\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ditributions \n",
    "The first step to understand the data is to plot it. Here we plot (1) every value of every feature, (2) the histogram and (3) the distribution for each feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot all values\n",
    "Plotting all the values can be usefull to determine if and how many outliers there are and also identify the categorical features. In these plots we consider both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(data_merged, col_labels = column_labels(), title=\"values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are very few outliers and that, given the amount of data, they shouldn't affect much the computation of the mean and of the standard deviation. Moreover, the only categorical feature is PRI_jet_num (column 22), we will study further what this feature represents and if/how it affect other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the histograms and the distributions\n",
    "In the graps below we plotted the histogram (left column) and the distribution (rigth column) of each feature depending on the output. In this way, we can compare the distribution of each feature and check by eye if it changes depending on the output y. In these graphs we can consider only the training data since we need the value of the output y to split the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(data_loaded, y_loaded, col_labels = column_labels(), title = \"\", normed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are the distribution of the four features PRI_tau_phi, PRI_lep_pt, PRI_lep_phi, PRI_met_phi (respectively columns 15, 16, 18, 20) seems to be independent from the output y. We will try to drop these columns and verify how it affects out model. Finally, we notice the precence of lot of -999 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the merged dataset\n",
    "x_all, _ = clean_input_data(data_merged.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all[0].shape[0] + x_all[1].shape[0] +  x_all[2].shape[0] + x_all[3].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = fill_with_nan_list(data_merged, nan_values=[-999])\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0, 101, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]\n",
    "# gerarchical maps: jet -> column -> percentile\n",
    "percentiles = {} \n",
    "for jet, d in enumerate(x_all):\n",
    "    percentiles[jet] = {} \n",
    "    \n",
    "    scan_perc = list(range(0, 101, 5))\n",
    "    #col_perc = np.zeros((len(scan_perc), data_merged.shape[1]))\n",
    "    for col in range(d.shape[1]): # scan columns\n",
    "        percentiles[jet][col] = {}\n",
    "        for p in scan_perc:\n",
    "             percentiles[jet][col][p] = np.nanpercentile(d[:, col], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the computed percentiles \n",
    "file_path = \"percentiles.json\"\n",
    "json.dump(percentiles, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our approach: split data depending on the feature PRI_jet_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why divide data depending on jet numbers and which columns can we drop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading information on the dataset we learned that the jet number affects the presence of -999 (invalid) values in other features. Therefore, we splitted our input data in four sets depending on the jet number and verified it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data depending on the jet number (column 22) that is a categorical number in {0, 1, 2, 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data_merged.copy()\n",
    "jets_0 = all_data[all_data[:, 22]==0, :]\n",
    "jets_1 = all_data[all_data[:, 22]==1, :]\n",
    "jets_2 = all_data[all_data[:, 22]==2, :]\n",
    "jets_3 = all_data[all_data[:, 22]==3, :]\n",
    "jets_0.shape, jets_1.shape, jets_2.shape, jets_3.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the -999 values?\n",
    "- jet = 0: columns [4, 5, 6, 12, 23, 24, 25, 26, 27, 28] contain only -999 values, 26.1% of entries in the first column have -999\n",
    "- jet = 1: columns [4, 5, 6, 12, 26, 27, 28] contain only -999 values, 7.6% of entries in the first column have -999\n",
    "- jet = 2: 3% of entries in the first column have -999\n",
    "- jet = 3: 1.4% of entries in the first column have -999\n",
    "\n",
    "We decided to drop in every set the columns that store only -999 values (since they do not keep any information). However, the first feature (DER_mass_MMC) is the only one that contains -999 values in all the 4 sets without filling completely the column. We tried to impute this invalid value with mean, std, median and also simply subtituted it with 0s (after standardization). After several trials we understood that this invalid value didn't \"fit\" in the column and therefore it could need a different weight from the other values of the same column. Therefore, we opted for the more versatile solution: add a boolean column to indicate the position of the -999 values and delete them from first column. By \"delete\" we mean: substitute them with 0s after standardization, where during standardization we ignored those invalid values so that they didn't affect mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet, cur_set in enumerate([jets_0, jets_1, jets_2, jets_3]):\n",
    "    print(\"Features in the dataset with jet=\", jet, \"contains this many values != -999\")\n",
    "    for col in range(30):\n",
    "        print(col, np.sum(cur_set[:, col] != -999))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the 0 values?\n",
    "- jet = 0: column 29 contains only 0s and, obviously, column 22 too since it stores the jet num.\n",
    "- jet = 1: spread\n",
    "- jet = 2: spread\n",
    "- jet = 3: spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet, cur_set in enumerate([jets_0, jets_1, jets_2, jets_3]):\n",
    "    print(\"Features in the dataset with jet=\", jet, \"contains this many values != 0\")\n",
    "    for col in range(30):\n",
    "        print(col, np.sum(cur_set[:, col] != 0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this first step we surely want to drop the following columns since they do not contain any useful information:\n",
    "- jet = 0: [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "- jet = 1: [4, 5, 6, 12, 22, 26, 27, 28] \n",
    "- jet = 2: [22]\n",
    "- jet = 3: [22]\n",
    "\n",
    "The column 22 is dropped in every obtained dataset since it just stores a constant representing the jet number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data and compute the correlation matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now divide our data, drop the above columns and verify if there are some highly correlated features. If so, it is worth trying to drop all but 1 column in a set of correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 0 columns dropped: [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "Jet 1 columns dropped: [4, 5, 6, 12, 22, 26, 27, 28]\n",
      "Jet 2 columns dropped: [22]\n",
      "Jet 3 columns dropped: [22]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((327371, 18), (252882, 22), (165027, 29), (72958, 29))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data\n",
    "datasets, _ = split_input_data(data_merged) # split and drop\n",
    "datasets[0].shape, datasets[1].shape, datasets[2].shape, datasets[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18, 18), (22, 22), (29, 29), (29, 29))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute correlation matrices\n",
    "corr_matrices = [None]*4\n",
    "for jet in range(4):\n",
    "    # don't consider the first column since it contains nan values (we will simply keep that column)\n",
    "    corr_matrices[jet] = np.corrcoef(datasets[jet][:, 1:].T) \n",
    "    \n",
    "    # to keep the same indexing of the columns just add one row above and one column at the left\n",
    "    corr_matrices[jet] = np.column_stack((np.zeros((corr_matrices[jet].shape[0], 1)), corr_matrices[jet]))   \n",
    "    corr_matrices[jet] = np.row_stack((np.zeros((1, corr_matrices[jet].shape[1])), corr_matrices[jet]))\n",
    "\n",
    "corr_matrices[0].shape, corr_matrices[1].shape, corr_matrices[2].shape, corr_matrices[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mapping of |correlations| > min_corr \n",
    "min_correlations = [0.7 ,  0.75,  0.8 ,  0.85,  0.9 ,  0.95]\n",
    "corr_mappings = {} # mapping: jet -> minimum correlation -> features -> list of correlated features\n",
    "for jet in range(4): # for each dataset build the correlation mapping\n",
    "    corr_mappings[jet] = {}\n",
    "    for min_corr in min_correlations: #for each min correlation considered\n",
    "        corr_mappings[jet][min_corr] = {}\n",
    "        corr_matrix_bool = np.abs(corr_matrices[jet]) > min_corr \n",
    "        nfeature = corr_matrix_bool.shape[0]\n",
    "        # i is surely correlated to itself, drop that (useless) information\n",
    "        for i in range(nfeature):\n",
    "            corr_matrix_bool[i][i] = False\n",
    "\n",
    "        # compute the mapping of correlations\n",
    "        for i in range(nfeature):\n",
    "            c = np.where(corr_matrix_bool[i])[0].tolist()\n",
    "            if len(c) > 0: # if it is not correlated to any other column then ignore it\n",
    "                corr_mappings[jet][min_corr][i] = c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0.7: {1: [15], 3: [5], 5: [3], 6: [9, 12], 9: [6], 12: [6], 15: [1]},\n",
       "  0.75: {3: [5], 5: [3], 6: [9, 12], 9: [6], 12: [6]},\n",
       "  0.8: {3: [5], 5: [3]},\n",
       "  0.85: {3: [5], 5: [3]},\n",
       "  0.9: {3: [5], 5: [3]},\n",
       "  0.95: {3: [5], 5: [3]}},\n",
       " 1: {0.7: {3: [6, 17, 18, 21],\n",
       "   6: [3, 17, 18, 21],\n",
       "   7: [12],\n",
       "   12: [7],\n",
       "   17: [3, 6, 18, 21],\n",
       "   18: [3, 6, 17, 21],\n",
       "   21: [3, 6, 17, 18]},\n",
       "  0.75: {3: [6, 18, 21],\n",
       "   6: [3, 17, 18, 21],\n",
       "   17: [6],\n",
       "   18: [3, 6, 21],\n",
       "   21: [3, 6, 18]},\n",
       "  0.8: {3: [6, 18, 21], 6: [3, 18, 21], 18: [3, 6, 21], 21: [3, 6, 18]},\n",
       "  0.85: {3: [6, 18, 21], 6: [3, 18, 21], 18: [3, 6, 21], 21: [3, 6, 18]},\n",
       "  0.9: {3: [18, 21], 6: [18, 21], 18: [3, 6, 21], 21: [3, 6, 18]},\n",
       "  0.95: {18: [21], 21: [18]}},\n",
       " 2: {0.7: {3: [9, 19, 21, 22, 28],\n",
       "   4: [5, 6],\n",
       "   5: [4, 6],\n",
       "   6: [4, 5],\n",
       "   9: [3, 21, 22, 28],\n",
       "   10: [16],\n",
       "   16: [10],\n",
       "   19: [3],\n",
       "   21: [3, 9, 22, 28],\n",
       "   22: [3, 9, 21, 28],\n",
       "   25: [28],\n",
       "   28: [3, 9, 21, 22, 25]},\n",
       "  0.75: {3: [9, 22, 28],\n",
       "   4: [5, 6],\n",
       "   5: [4, 6],\n",
       "   6: [4, 5],\n",
       "   9: [3, 21, 22, 28],\n",
       "   21: [9, 22, 28],\n",
       "   22: [3, 9, 21, 28],\n",
       "   28: [3, 9, 21, 22]},\n",
       "  0.8: {4: [5, 6],\n",
       "   5: [4],\n",
       "   6: [4],\n",
       "   9: [21, 22, 28],\n",
       "   21: [9, 22, 28],\n",
       "   22: [9, 21, 28],\n",
       "   28: [9, 21, 22]},\n",
       "  0.85: {4: [6], 6: [4], 9: [21, 22, 28], 21: [9], 22: [9, 28], 28: [9, 22]},\n",
       "  0.9: {9: [22, 28], 22: [9, 28], 28: [9, 22]},\n",
       "  0.95: {22: [28], 28: [22]}},\n",
       " 3: {0.7: {3: [19],\n",
       "   4: [5, 6],\n",
       "   5: [4],\n",
       "   6: [4],\n",
       "   9: [21, 22, 25, 28],\n",
       "   10: [16],\n",
       "   16: [10],\n",
       "   19: [3],\n",
       "   21: [9, 22, 25, 28],\n",
       "   22: [9, 21, 28],\n",
       "   25: [9, 21, 28],\n",
       "   28: [9, 21, 22, 25]},\n",
       "  0.75: {4: [5, 6],\n",
       "   5: [4],\n",
       "   6: [4],\n",
       "   9: [21, 22, 25, 28],\n",
       "   10: [16],\n",
       "   16: [10],\n",
       "   21: [9, 22, 28],\n",
       "   22: [9, 21, 28],\n",
       "   25: [9, 28],\n",
       "   28: [9, 21, 22, 25]},\n",
       "  0.8: {9: [21, 22, 28],\n",
       "   21: [9, 22, 28],\n",
       "   22: [9, 21, 28],\n",
       "   25: [28],\n",
       "   28: [9, 21, 22, 25]},\n",
       "  0.85: {9: [21, 22, 28], 21: [9, 28], 22: [9, 28], 28: [9, 21, 22]},\n",
       "  0.9: {9: [21, 28], 21: [9], 28: [9]},\n",
       "  0.95: {9: [28], 28: [9]}}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the mappings of the correlated columns we compute the columns that could be dropped for each considered correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0.7: [5, 9, 12, 15],\n",
       "  0.75: [5, 9, 12],\n",
       "  0.8: [5],\n",
       "  0.85: [5],\n",
       "  0.9: [5],\n",
       "  0.95: [5]},\n",
       " 1: {0.7: [6, 12, 17, 18, 21],\n",
       "  0.75: [3, 17, 18, 21],\n",
       "  0.8: [6, 18, 21],\n",
       "  0.85: [6, 18, 21],\n",
       "  0.9: [3, 6, 21],\n",
       "  0.95: [21]},\n",
       " 2: {0.7: [5, 6, 9, 16, 19, 21, 22, 28],\n",
       "  0.75: [3, 5, 6, 21, 22, 28],\n",
       "  0.8: [5, 6, 21, 22, 28],\n",
       "  0.85: [6, 21, 22, 28],\n",
       "  0.9: [22, 28],\n",
       "  0.95: [28]},\n",
       " 3: {0.7: [5, 6, 16, 19, 21, 22, 25, 28],\n",
       "  0.75: [5, 6, 16, 21, 22, 25, 28],\n",
       "  0.8: [9, 21, 22, 25],\n",
       "  0.85: [21, 22, 28],\n",
       "  0.9: [21, 28],\n",
       "  0.95: [28]}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def empty(map_):\n",
    "    for k in map_:\n",
    "        if len(map_[k]) > 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# jet -> minimum correlation -> features -> list of correlated features\n",
    "\n",
    "tobe_deleted = {} # mapping: jet -> minimum correlation -> list of columns that can be dropped\n",
    "for jet in range(4): # for each dataset build the correlation mapping\n",
    "    tobe_deleted[jet] = {}\n",
    "    for min_corr, corr in corr_mappings[jet].items():\n",
    "        tobe_deleted[jet][min_corr] = []\n",
    "        # fetch all the columns that can be deleted and put them in tobe_deleted\n",
    "        while not empty(corr): \n",
    "            longer_key = -1 \n",
    "            longer_length = 0\n",
    "\n",
    "            # look for the longer list\n",
    "            for key in corr:\n",
    "                curr_length = len(corr[key])\n",
    "                if curr_length > longer_length:\n",
    "                    longer_length = curr_length\n",
    "                    longer_key = key\n",
    "\n",
    "            tobe_deleted[jet][min_corr].append(corr[longer_key])\n",
    "            # delete all the columns that are correlated to column longer_key\n",
    "            # i.e. all the column whose index is in  corr[longer_key]\n",
    "            for corr_colum in corr[longer_key]:\n",
    "                corr[corr_colum] = []\n",
    "\n",
    "            # since those columns have been dropped they must be removed from all the other lists\n",
    "            for key in corr: \n",
    "                if key != longer_key:\n",
    "                    corr[key] = list(set(corr[key]) - set(corr[longer_key]))\n",
    "            corr[longer_key] = []\n",
    "\n",
    "        tobe_deleted[jet][min_corr] = [val for sublist in tobe_deleted[jet][min_corr] for val in sublist]\n",
    "        tobe_deleted[jet][min_corr].sort()\n",
    "tobe_deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The found maps show, for evey dataset and for every chosen minimum correlation a list of features that can be dropped, e.g. tobe_deleted[2][0.85] contains a list of features in the jet=2 dataset which have a |correlation| > 0.85 with at least one of the feature that is kept in the dataset (and therefore can be dropped)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must use the same standardisation process both for the training and for predicting. Since the -999 values will be dropped from the first column I remove them to compute the mean and the std."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
