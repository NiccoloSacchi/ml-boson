{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import codecs, json \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_tr removing all samples with -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import some data to test the functions\n",
    "from helpers import *\n",
    "\n",
    "ids_tr,predictions_tr,data_tr, ids_te,data_te = load_boson_data()\n",
    "\n",
    "invalid_indexes= np.unique(np.where(data_tr == -999)[0])  \n",
    "invalid_indexes   #\"indexes\" of the rows where -999 is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data_tr = np.delete(data_tr,np.unique(np.where(data_tr == -999)[0]),0)\n",
    "\n",
    "clean_data_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_tr replacing -999 with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with_zero_data_tr = np.where(data_tr == -999, 0, data_tr)\n",
    "\n",
    "with_zero_data_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_tr replacing -999 with the mean/median/mfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some analysys with the help of pandas and seaborn\n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "ndata = 100\n",
    "data_path = \"../dataset/train.csv\"\n",
    "_, data_loaded, _ = load_partial_csv_data(data_path, ndata)\n",
    "print(data_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_ = data_loaded\n",
    "# x_, ndropped = drop_corr_columns(data_loaded, 0.5)\n",
    "x_ = pd.DataFrame(x_)\n",
    "x_.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize distribution of features and pairwise relationship using seaborn pairplot\n",
    "sbn.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute and print the pairwise correlation between the columns in data frame\n",
    "\n",
    "correlations = data.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that have a strong correlation with other fatures:\n",
    "* DER_tp_h\n",
    "* DER_deltaeta_jet_jet\n",
    "* DER_mass_jet_jet\n",
    "* DER_prodeta_jet_jet\n",
    "* DER_sum_pt\n",
    "* DER_met_phi_centrality\n",
    "* DER_lep_eta_centrality\n",
    "* PRI_met_sumet\n",
    "* PRI_jet_num\n",
    "* PRI_jet_leading_pt\n",
    "* PRI_jet_leading_eta\n",
    "* PRI_jet_leading_phi\n",
    "* PRI_jet_subleading_pt\n",
    "* PRI_jet_subleading_eta\n",
    "* PRI_jet_subleading_phi\n",
    "* PRI_jet_all_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "data_path = \"../dataset/train.csv\"\n",
    "y_loaded, data_loaded, _ = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "data_path = \"../dataset/test.csv\"\n",
    "_, data_test_loaded, _ = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 30), (568238, 30))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaded.shape, data_test_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged= np.concatenate((data_loaded,data_test_loaded), axis=0)\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from implementations import *\n",
    "data2 = standardize_final(data_merged)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.17216387e-09,   1.03710716e-09,  -9.49157558e-10,\n",
       "         2.75214382e-10,  -2.37154677e-09,  -4.24695293e-10,\n",
       "         1.26353578e-10,   2.77264806e-09,  -2.20539680e-09,\n",
       "        -6.19460966e-10,   5.35282624e-09,  -2.53746009e-10,\n",
       "        -1.01104676e-09,   2.82057741e-10,  -2.95827309e-12,\n",
       "         7.31771013e-12,   2.82354029e-10,  -4.66231381e-12,\n",
       "        -3.33055435e-12,  -1.10220055e-09,  -2.13645860e-12,\n",
       "         2.09908730e+02,   6.26473392e-10,   5.67837831e-10,\n",
       "         8.28088141e-13,  -9.15971787e-12,   1.20276071e-09,\n",
       "         1.60110842e-12,   1.99444829e-11,  -2.67166579e-09])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(data2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLEAN DATA: YOU MAY WANT TO CHAGE THIS STEP\n",
    "#data = drop_columns_with_70_nan_ratio(data_merged)\n",
    "data = fill_with_nan_list(data_merged, nan_values=[0, -999])\n",
    "#data = sustitute_nans(data, substitutions=np.nanmean(data, axis=0))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.14019075e+00,   1.39218419e+00,   1.99938878e+00,\n",
       "          9.12408517e-01,   1.38027241e+00,   9.34577485e-01,\n",
       "         -2.31344554e-01,   3.04001515e+00,   8.65575236e-01,\n",
       "          1.36615140e+00,   1.70247376e+00,  -1.06587704e-01,\n",
       "          1.63461178e+00,   1.72536214e+00,  -9.61087031e-03,\n",
       "         -7.24585534e-03,   2.10874050e+00,  -1.50926450e-02,\n",
       "          2.72513983e-02,   1.28183183e+00,  -4.76360367e-03,\n",
       "          2.09908730e+02,   2.24488816e+00,   1.39991806e+00,\n",
       "         -7.01422387e-04,  -1.03879884e-02,   1.78122254e+00,\n",
       "         -3.28284799e-03,  -5.76559837e-03,   1.21063717e+00]),\n",
       " array([   1.        ,    1.        ,    1.        ,    1.        ,\n",
       "           1.        ,    1.        ,    1.        ,    1.        ,\n",
       "           1.        ,    1.        ,    1.        ,    1.        ,\n",
       "           1.        ,    1.        ,    1.        ,    1.        ,\n",
       "           1.        ,    1.        ,    1.        ,    1.        ,\n",
       "           1.        ,  126.81660841,    1.        ,    1.        ,\n",
       "           1.        ,    1.        ,    1.        ,    1.        ,\n",
       "           1.        ,    1.        ]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "means = np.nanmean(data, axis=0)\n",
    "std_devs = np.nanstd(data, axis=0)\n",
    "\n",
    "data_std = data - means\n",
    "data_std = data / whole_data_std_devs()\n",
    "\n",
    "means = np.nanmean(data_std, axis=0)\n",
    "std_devs = np.nanstd(data_std, axis=0)\n",
    "\n",
    "means,std_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.21867697e+02,   4.92532558e+01,   8.11405610e+01,\n",
       "          5.78582923e+01,   2.40500975e+00,   3.72181050e+02,\n",
       "         -8.29392140e-01,   2.37387138e+00,   1.89724461e+01,\n",
       "          1.58596159e+02,   1.43877554e+00,  -1.27303822e-01,\n",
       "          5.85210606e-01,   3.86981522e+01,  -1.16628927e-02,\n",
       "         -1.31600006e-02,   4.66924138e+01,  -1.90822693e-02,\n",
       "          4.94674822e-02,   4.16545265e+01,  -8.63571168e-03,\n",
       "          2.09908730e+02,   1.63345672e+00,   8.49042850e+01,\n",
       "         -1.24824003e-03,  -1.88594668e-02,   5.78102860e+01,\n",
       "         -6.67041978e-03,  -1.04712859e-02,   1.22028164e+02]),\n",
       " [121.867697,\n",
       "  49.2532558,\n",
       "  81.140561,\n",
       "  57.8582923,\n",
       "  2.40500975,\n",
       "  372.18105,\n",
       "  -0.82939214,\n",
       "  2.37387138,\n",
       "  18.9724461,\n",
       "  158.596159,\n",
       "  1.43877554,\n",
       "  -0.127303822,\n",
       "  0.585210606,\n",
       "  38.6981522,\n",
       "  -0.0116628927,\n",
       "  -0.0131600006,\n",
       "  46.6924138,\n",
       "  -0.0190822693,\n",
       "  0.0494674822,\n",
       "  41.6545265,\n",
       "  -0.00863571168,\n",
       "  209.90873,\n",
       "  1.63345672,\n",
       "  84.904285,\n",
       "  -0.00124824003,\n",
       "  -0.0188594668,\n",
       "  57.810286,\n",
       "  -0.00667041978,\n",
       "  -0.0104712859,\n",
       "  122.028164])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_means = whole_data_means()\n",
    "raw_std_devs = whole_data_std_devs()\n",
    "\n",
    "means,raw_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMPUTE AND STORE CORRELATION MATRIX\n",
    "corr_matrix = np.abs(np.corrcoef(data.T)) \n",
    "file_path = \"corr_matrix23.json\"\n",
    "json.dump(corr_matrix.tolist(), codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example: LOAD CORRELATION MATRIX\n",
    "file_path = \"corr_matrix23.json\"\n",
    "obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "b_new = json.loads(obj_text)\n",
    "corr_matrix_loaded = np.array(b_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exaple: GET CORRELATED COLUMNS\n",
    "corr_matrix = np.abs(corr_matrix) > 0.6\n",
    "# remove i from the list (i is surely correlated to himself)\n",
    "for i in range(23):\n",
    "    corr_matrix[i][i] = False\n",
    "    \n",
    "correlated_to = {}\n",
    "for i in range(23):\n",
    "    c = np.where(corr_matrix[i])[0]\n",
    "    if len(c) > 0: # if it is not correlated to any other column then ignore it\n",
    "        correlated_to[i] = c\n",
    "correlated_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ditributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot data as it is\n",
    "plot_distributions(data_loaded, y_loaded, col_labels = column_labels(), title = \"distribution_rawData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot data after cleaning it is\n",
    "x_cleaned, keptCols = clean_x(data_loaded, 0.7, subs_func = np.nanmean)\n",
    "\n",
    "plot_distributions(x_cleaned, y_loaded, col_labels = keptCols, title = \"distribution_cleanedData_999=mean_0=mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot data after cleaning it is\n",
    "x_cleaned, keptCols = clean_x(data_loaded, 0.7, subs_func = None)\n",
    "\n",
    "plot_distributions(x_cleaned, y_loaded, col_labels = keptCols, title = \"distribution_cleanedData_999=999_0=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# better norm the hist to have a clearer idea of the proportions\n",
    "f, ax = plt.subplots(2, 2)\n",
    "dataset1 = [0, 0, 0, 0, 1, 2, 4, 6]\n",
    "dataset2 = [0, 0, 1, 6]\n",
    "\n",
    "ax[0][0].hist(dataset1, histtype='step',color=\"red\", normed=True)\n",
    "ax[0][1].hist(dataset1, histtype='step',color=\"red\", normed=False)\n",
    "\n",
    "\n",
    "ax[1][0].hist(dataset2, histtype='step', color=\"blue\", normed=True)\n",
    "ax[1][1].hist(dataset2, histtype='step', color=\"blue\", normed=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -999 values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there are three groups of columns with the -999 values in the same positions\n",
    "print(np.sum(x_loaded == -999, axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(data_loaded)\n",
    "\n",
    "pri = np.where(column_labels() == \"PRI_jet_num\")\n",
    "dropped = list(set(range(30))- set([0,  1,  2,  3,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29]))\n",
    "d = data[np.any(data==-999, axis=1)].loc[:, np.append(dropped ,pri)]\n",
    "\n",
    "d[d[22] <= 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d[d[22] > 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_features(data_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = fill_with_nan_list(data_merged, nan_values=[0, -999])\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 101, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 30)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]\n",
    "scan_perc = list(range(0, 101, 5))\n",
    "col_perc = np.zeros((len(scan_perc), data_merged.shape[1]))\n",
    "for col in range(data_merged.shape[1]):\n",
    "    for i, p in enumerate(scan_perc):\n",
    "         col_perc[i, col] = np.nanpercentile(data_merged[:, col], p)\n",
    "col_perc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store both the computed percentiles and the list of % to which they correspond\n",
    "file_path = \"percentiles.json\"\n",
    "map_ = {\n",
    "    \"scan_perc\": scan_perc,\n",
    "    \"col_perc\": col_perc.tolist()\n",
    "}\n",
    "json.dump(map_, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approach: split data depending on jet number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why divide data depending on jet numbers and which columns can we drop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: most of the -999 value are associated with jet number <= 1. The only feature that still contains some -999 values is the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = data_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data depending on the jet number (column 22) that is a categorical number in {0, 1, 2, 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((327371, 30), (252882, 30), (165027, 30), (72958, 30))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jets_0 = all_data[all_data[:, 22]==0, :]\n",
    "jets_1 = all_data[all_data[:, 22]==1, :]\n",
    "jets_2 = all_data[all_data[:, 22]==2, :]\n",
    "jets_3 = all_data[all_data[:, 22]==3, :]\n",
    "jets_0.shape, jets_1.shape, jets_2.shape, jets_3.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the -999 values?\n",
    "- jet = 0: columns [4, 5, 6, 12, 23, 24, 25, 26, 27, 28] contain only -999 values, 26.1% of entries in the first column have -999\n",
    "- jet = 1: columns [4, 5, 6, 12, 26, 27, 28] contain only -999 values, 7.6% of entries in the first column have -999\n",
    "- jet = 2: 3% of entries in the first column have -999\n",
    "- jet = 3: 1.4% of entries in the first column have -999\n",
    "\n",
    "The two tables relative to jet 2 and jet 3 could be merged. The -999 values in the first column will be removed from that column and a boolean column will be created to indicate the position of the -999 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for jet, cur_set in enumerate([jets_0, jets_1, jets_2, jets_3]):\n",
    "    print(\"Features in the dataset with jet=\", jet, \"contains this many values != -999\")\n",
    "    for col in range(30):\n",
    "        print(col, np.sum(cur_set[:, col] != -999))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the 0 values?\n",
    "- jet = 0: column 29 contains only 0s. (and, obviously, column 22 since it stores the jet num)\n",
    "- jet = 1: spread\n",
    "- jet = 2: spread\n",
    "- jet = 3: spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for jet, cur_set in enumerate([jets_0, jets_1, jets_2, jets_3]):\n",
    "    print(\"Features in the dataset with jet=\", jet, \"contains this many values != 0\")\n",
    "    for col in range(30):\n",
    "        print(col, np.sum(cur_set[:, col] != 0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this first step we surely want to drop the following columns since they do not contain any useful information:\n",
    "- jet = 0: [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "- jet = 1: [4, 5, 6, 12, 22, 26, 27, 28] \n",
    "- jet = 2: [22]\n",
    "- jet = 3: [22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data and compute the correlation matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now divide our data, drop the above columns and verify if there are some highly correlated features. If so, it is worth trying to drop all but 1 column in a set of correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((327371, 18), (252882, 22), (165027, 29), (72958, 29))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = split_input_data(data_merged) # split and drop\n",
    "datasets[0].shape, datasets[1].shape, datasets[2].shape, datasets[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17, 17), (21, 21), (28, 28), (28, 28))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrices = [None]*4\n",
    "for jet in range(4):\n",
    "    # don't consider the first column since it contains nan values (we will simply keep that column)\n",
    "    corr_matrices[jet] = np.corrcoef(datasets[jet][:, 1:].T) \n",
    "corr_matrices[0].shape, cor_matrices[1].shape, cor_matrices[2].shape, cor_matrices[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}, {}, {}, {}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{}]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the mapping of correlations > min_corr\n",
    "min_corr = 0.7\n",
    "corr_mappings = [{}]*4\n",
    "for jet in range(4):\n",
    "    corr_matrix_bool = np.abs(corr_matrices[jet]) > min_corr \n",
    "    nfeature = corr_matrix_bool.shape[0]\n",
    "    # i is surely correlated to itself, drop that (useless) information\n",
    "    for i in range(nfeature):\n",
    "        corr_matrix_bool[i][i] = False\n",
    "\n",
    "    # compute the mapping of correlations\n",
    "    for i in range(nfeature):\n",
    "        c = np.where(corr_matrix_bool[i])[0].tolist()\n",
    "        if len(c) > 0: # if it is not correlated to any other column then ignore it\n",
    "            corr_mappings[jet][i] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [14],\n",
       " 2: [18],\n",
       " 3: [4, 5],\n",
       " 4: [3],\n",
       " 5: [3],\n",
       " 6: [11],\n",
       " 8: [20, 21, 24, 27],\n",
       " 9: [15],\n",
       " 11: [6],\n",
       " 14: [0],\n",
       " 15: [9],\n",
       " 16: [2, 5, 17, 20],\n",
       " 17: [2, 5, 16, 20],\n",
       " 18: [2],\n",
       " 20: [8, 21, 24, 27],\n",
       " 21: [8, 20, 27],\n",
       " 24: [8, 20, 27],\n",
       " 27: [8, 20, 21, 24]}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mappings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must use the same standardisation process both for the training and for predicting. Since the -999 values will be dropped from the first column I remove them to compute the mean and the std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((327371, 18), (252882, 22), (165027, 29), (72958, 29))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = split_input_data(data_merged)\n",
    "datasets[0].shape, datasets[1].shape, datasets[2].shape, datasets[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_jet0 = np.corrcoef(datasets[0][:, 1:].T)\n",
    "corr_jet0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "d = datasets[0]\n",
    "for col in range(d.shape[1]):\n",
    "    print(np.all(d[:, col]==d[:, col][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
