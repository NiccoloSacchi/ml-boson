{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from plots import visualization\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from helpers import equalize_predictions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load boson data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just load the training dataset\n",
    "data_path = \"../dataset/train.csv\"\n",
    "y_loaded, x_loaded, ids_te = load_csv_data(data_path, sub_sample=False)\n",
    "y_loaded = y_loaded.reshape((-1, 1))\n",
    "y_loaded.shape, x_loaded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loaded.shape, y_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, y_all = clean_input_data(x_loaded.copy(), y_loaded.copy(), corr=1)\n",
    "for i in range(len(y_all)):\n",
    "    y_all[i][y_all[i]== -1] = 0\n",
    "x_all[0].shape, x_all[1].shape, x_all[2].shape, x_all[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all[0].shape, y_all[1].shape, y_all[2].shape, y_all[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check itis 250000\n",
    "x_all[0].shape[0] + x_all[1].shape[0] +  x_all[2].shape[0] + x_all[3].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(np.sum(np.isnan(x_all[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Choose the degree and set the gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "# choose degree\n",
    "degree = 3\n",
    "#tx_sub = build_poly(x_sub, degree)\n",
    "tx_all = build_poly(x_all[i], degree)\n",
    "tx_all.shape, y_all[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncolumns = x_all[i].shape[1]\n",
    "\n",
    "# 50000 data, 14 columns, 999=mean, 0=mean\n",
    "gamma = np.concatenate([\n",
    "     # gamma for constant and 1st degree \n",
    "    np.ones(ncolumns+1)*1e-5,\n",
    "    # gammma 2nd degree\n",
    "    np.ones(ncolumns)*1e-6, \n",
    "    # gamma for 3rd degree \n",
    "    np.ones(ncolumns)*1e-7,\n",
    "#     # gamma for 4th degree\n",
    "#     np.ones(ncolumns)*1e-10,\n",
    "#     # gamma for 5th degree\n",
    "#     np.ones(ncolumns)*1e-12,\n",
    "#     # gamma for 6th degree\n",
    "#     np.ones(ncolumns)*1e-15,\n",
    "#     # gamma for 7th degree \n",
    "#     np.ones(ncolumns)*1e-17,\n",
    "#     # gamma for 8th degree \n",
    "#     np.ones(ncolumns)*1e-20,\n",
    "#     # gamma for 9th degree \n",
    "#     np.ones(ncolumns)*1e-24,\n",
    "#     # gamma for 10th degree \n",
    "#     np.ones(ncolumns)*1e-24,\n",
    "#     # gamma for 11th degree \n",
    "#     np.ones(ncolumns)*1e-28,\n",
    "])\\\n",
    ".reshape((-1, 1))*0.05\n",
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for clean_x3\n",
    "n_bool_cols = bool_cols.shape[1]\n",
    "n_other_cols = x_all.shape[1]\n",
    "\n",
    "# 50000 data, 14 columns, 999=mean, 0=mean\n",
    "gamma = np.concatenate([\n",
    "    # gamma for bool cols\n",
    "    np.ones(n_bool_cols)*1e-5,\n",
    "     # gamma for constant and 1st degree \n",
    "    np.ones(n_other_cols+1)*1e-5,\n",
    "    # gammma 2nd degree\n",
    "    np.ones(n_other_cols)*1e-6, \n",
    "    # gamma for 3rd degree \n",
    "    np.ones(n_other_cols)*1e-7,\n",
    "    # gamma for 4th degree\n",
    "    np.ones(n_other_cols)*1e-9,\n",
    "    # gamma for 5th degree\n",
    "    np.ones(n_other_cols)*1e-13,\n",
    "#     # gamma for 6th degree\n",
    "#     np.ones(n_other_cols)*1e-16,\n",
    "#     # gamma for 7th degree \n",
    "#     np.ones(n_other_cols)*1e-19,\n",
    "#     # gamma for 8th degree \n",
    "#     np.ones(n_other_cols)*1e-20,\n",
    "#     # gamma for 9th degree \n",
    "#     np.ones(n_other_cols)*1e-24,\n",
    "#     # gamma for 10th degree \n",
    "#     np.ones(n_other_cols)*1e-24,\n",
    "#     # gamma for 11th degree \n",
    "#     np.ones(n_other_cols)*1e-28,\n",
    "])\\\n",
    ".reshape((-1, 1))*0.5\n",
    "\n",
    "gamma.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "max_iters = 142\n",
    "ouptut_step = 100\n",
    "loss, w = gradient_descent(y_all[i], tx_all, initial_w, max_iters, gamma, num_batches=1,\n",
    "                           plot_losses=True, print_output=True, ouptut_step=50, costfunc=CostFunction.LIKELIHOOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loss(y_all[i], tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "max_iters = 500\n",
    "ouptut_step = 100\n",
    "lambda_ = 1\n",
    "loss, w = gradient_descent(y_all[i], tx_all, initial_w, max_iters, gamma, num_batches=1, lambda_=lambda_,\n",
    "                           plot_losses=True, print_output=True, ouptut_step=50, costfunc=CostFunction.LIKELIHOOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loss(y_all[i], tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create submit file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../dataset/test.csv\"\n",
    "y_te_loaded, x_te_loaded, ids_te_loaded = load_csv_data(data_path, sub_sample=False)\n",
    "y_te_loaded.shape, x_te_loaded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_te[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean, have to split the ids too otherwise we lose the mappings\n",
    "x_te, ids_te = clean_input_data(x_te_loaded.copy(), ids_te_loaded.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Build the polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build\n",
    "degree = 3\n",
    "tx_te = []\n",
    "for x_te_ in x_te:\n",
    "    tx_te.append(build_poly(x_te_, degree))\n",
    "tx_te[0].shape, tx_te[1].shape, tx_te[2].shape, tx_te[3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  d. Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te_pred = []\n",
    "for jet in range(4):\n",
    "    y_te_pred.append(predict_labels(weigths[jet], tx_te[jet]))\n",
    "y_te_pred[0].shape, y_te_pred[1].shape, y_te_pred[2].shape, y_te_pred[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the results\n",
    "for jet in range(4):\n",
    "    ids_te[jet] = ids_te[jet].reshape((-1, 1))\n",
    "y_pred = np.row_stack([y_te_pred[0], y_te_pred[1], y_te_pred[2], y_te_pred[3]])\n",
    "ids = np.row_stack([ids_te[0], ids_te[1], ids_te[2], ids_te[3]])\n",
    "y_pred.shape, ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it makes sense\n",
    "(y_pred==-1).sum(), (y_pred==1).sum()\n",
    "# (393191, 175047)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Store predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predictions\n",
    "sub_file_name = \"logistic_regression_definitiveClean_degree3\"\n",
    "create_csv_submission(ids, y_pred, sub_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store/load weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigth_name = \"w3\"\n",
    "#file_path = \"weigths/no_perc/w1\"\n",
    "file_path = \"weigths/with_perc3/\" + weigth_name\n",
    "json.dump(w.tolist(), codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigths = []\n",
    "for weigth_name in [\"w0\", \"w1\", \"w2\", \"w3\"]:\n",
    "    file_path = \"weigths/with_perc3/\"+weigth_name\n",
    "    obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "    weigths.append(np.array(json.loads(obj_text)))\n",
    "weigths[0].shape, weigths[1].shape, weigths[2].shape, weigths[3].shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
