{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import visualization\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from helpers import equalize_predictions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_ex5 import sample_data, load_data \n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=500)\n",
    "x, mean_x, std_x = standardize(X)\n",
    "y.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub = y\n",
    "x_sub = build_poly(x, 1)\n",
    "y_sub.shape, x_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load boson data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 1), (250000, 30))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just load the training dataset\n",
    "data_path = \"../dataset/train.csv\"\n",
    "y_loaded, x_loaded, ids_te = load_csv_data(data_path, sub_sample=False)\n",
    "y_loaded = y_loaded.reshape((-1, 1))\n",
    "y_loaded.shape, x_loaded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide the maximum correlation between the columns\n",
    "corr = 0.7\n",
    "# clean the input features\n",
    "x_all, keptCols = clean_x(x_loaded, corr, subs_func=np.nanmean, bool_col=True)\n",
    "y_all = y_loaded.copy()\n",
    "y_all[y_all== -1] = 0\n",
    "\n",
    "# extract a subsample for the training\n",
    "subsample = 50000\n",
    "indices = np.random.RandomState(seed = 6).permutation(y_all.shape[0]) # get always the same random array\n",
    "x_sub, y_sub = x_all[indices[:subsample]], y_all[indices[:subsample]]\n",
    "\n",
    "x_sub.shape, y_sub.shape, x_all.shape, y_all.shape, keptCols.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the correlation matrix that was computed on the whole dataset\n",
    "file_path = \"corr_matrix.json\"\n",
    "obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "b_new = json.loads(obj_text)\n",
    "corr_matrix_loaded = np.array(b_new)\n",
    "corr_matrix_bool = np.abs(corr_matrix_loaded) > 0.85\n",
    "ncols = 30\n",
    "# i is surely correlated to itself, drop that (useless) information\n",
    "for i in range(ncols):\n",
    "    corr_matrix_bool[i][i] = False\n",
    "\n",
    "\n",
    "# compute the mapping of correlations\n",
    "corr = {} \n",
    "for i in range(ncols):\n",
    "    c = np.where(corr_matrix_bool[i])[0].tolist()\n",
    "    if len(c) > 0: # if it is not correlated to any other column then ignore it\n",
    "        corr[i] = c\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = y_loaded.copy()\n",
    "y_all[y_all== -1] = 0\n",
    "\n",
    "x_all = clean_x2(x_loaded, double=False)\n",
    "\n",
    "subsample = 50000\n",
    "indices = np.random.RandomState(seed = 6).permutation(y_all.shape[0]) # get always the same random array\n",
    "x_sub, y_sub = x_all[indices[:subsample]], y_all[indices[:subsample]]\n",
    "x_sub.shape, y_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x_loaded[:, 0]==-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.357\n"
     ]
    }
   ],
   "source": [
    "asd = x_loaded.copy()\n",
    "asd = fill_with_nan_list(asd, nan_values=[0, -999])\n",
    "col = 11\n",
    "c = asd[:, col]\n",
    "med = np.nanmedian(c)\n",
    "print(med)\n",
    "asd = np.insert(asd, -1, c*(c>=med), axis = 1)\n",
    "asd[:, col] = c*(c<med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(x_loaded[:, 3]>2500) #7343, 7343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.27697269,  1.29205176,  1.29205176, ...,  0.55066413,\n",
       "        1.29205176, -1.07452459])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, _, _ = standardize(c*(c>=med))\n",
    "x_all[:, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 boolean columns have been created.\n",
      "6 columns have been removed:  [9, 15, 18, 20, 22, 23]\n",
      "0 -0.0331691795196 124999 125001\n",
      "1 -0.0768266999163 124999 125001\n",
      "2 -0.181979788603 124996 125004\n",
      "3 -0.305090158949 125000 125000\n",
      "4 0.0 32434 217566\n",
      "5 0.0 23922 226078\n",
      "6 0.0 42490 207510\n",
      "7 0.151230950719 125000 125000\n",
      "8 -0.295694700715 124999 125001\n",
      "9 -0.186577169085 124943 125057\n",
      "10 -0.190723506948 124989 125011\n",
      "11 0.0 32114 217886\n",
      "12 -0.308022848817 124995 125005\n",
      "13 -0.00990625779749 124983 125017\n",
      "14 -0.278460962776 124996 125004\n",
      "15 -0.020148954398 124978 125022\n",
      "16 -0.210223828937 124996 125004\n",
      "17 -0.23761545611 124999 125001\n",
      "18 0.0 75105 174895\n",
      "19 0.0 74543 175457\n",
      "20 0.0 25031 224969\n",
      "21 0.0 36291 213709\n",
      "22 0.0 36256 213744\n",
      "23 -0.0331691795196 124999 125001\n",
      "0 0.0 82316 167684\n",
      "1 0.0 119315 130685\n",
      "2 0.0 95838 154162\n",
      "3 0.0 88788 161212\n",
      "4 0.0 32434 217566\n",
      "5 0.0 23922 226078\n",
      "6 0.0 42490 207510\n",
      "7 0.0759347970474 125000 125000\n",
      "8 0.0 110272 139728\n",
      "9 0.0 101234 148766\n",
      "10 0.0 119052 130948\n",
      "11 0.0 32114 217886\n",
      "12 0.0 86874 163126\n",
      "13 0.0 124219 125781\n",
      "14 0.0 90340 159660\n",
      "15 0.0 123904 126096\n",
      "16 0.0 96059 153941\n",
      "17 0.0 98354 151646\n",
      "18 0.0 75105 174895\n",
      "19 0.0 74543 175457\n",
      "20 0.0 25031 224969\n",
      "21 0.0 36291 213709\n",
      "22 0.0 36256 213744\n",
      "23 -0.0331691795196 124999 125001\n",
      "24 -0.0768266999163 124999 125001\n",
      "25 -0.181979788603 124996 125004\n",
      "26 -0.152549006806 125000 125000\n",
      "27 0.0 0 250000\n",
      "28 0.0 0 250000\n",
      "29 0.0 0 250000\n",
      "30 0.0 12638 237362\n",
      "31 -0.295694700715 124999 125001\n",
      "32 -0.186577169085 124943 125057\n",
      "33 -0.190723506948 124989 125011\n",
      "34 0.0 0 250000\n",
      "35 -0.308022848817 124995 125005\n",
      "36 -0.00990625779749 124983 125017\n",
      "37 -0.278460962776 124996 125004\n",
      "38 -0.020148954398 124978 125022\n",
      "39 -0.210223828937 124996 125004\n",
      "40 -0.23761545611 124999 125001\n",
      "41 0.0 0 250000\n",
      "42 0.0 0 250000\n",
      "43 0.0 0 250000\n",
      "44 0.0 0 250000\n",
      "45 0.0 0 250000\n",
      "46 -0.0331691795196 124999 125001\n",
      "47 0.0 0 250000\n",
      "0 0.0 82316 167684\n",
      "1 0.0 119315 130685\n",
      "2 0.0 95838 154162\n",
      "3 0.0 88788 161212\n",
      "4 0.0 32434 217566\n",
      "5 0.0 23922 226078\n",
      "6 0.0 42490 207510\n",
      "7 0.0759347970474 125000 125000\n",
      "8 0.0 110272 139728\n",
      "9 0.0 101234 148766\n",
      "10 0.0 119052 130948\n",
      "11 0.0 32114 217886\n",
      "12 0.0 86874 163126\n",
      "13 0.0 124219 125781\n",
      "14 0.0 90340 159660\n",
      "15 0.0 123904 126096\n",
      "16 0.0 96059 153941\n",
      "17 0.0 98354 151646\n",
      "18 0.0 75105 174895\n",
      "19 0.0 74543 175457\n",
      "20 0.0 25031 224969\n",
      "21 0.0 36291 213709\n",
      "22 0.0 36256 213744\n",
      "23 -0.0331691795196 124999 125001\n",
      "24 -0.0768266999163 124999 125001\n",
      "25 -0.181979788603 124996 125004\n",
      "26 -0.152549006806 125000 125000\n",
      "27 0.0 0 250000\n",
      "28 0.0 0 250000\n",
      "29 0.0 0 250000\n",
      "30 0.0 12638 237362\n",
      "31 -0.295694700715 124999 125001\n",
      "32 -0.186577169085 124943 125057\n",
      "33 -0.190723506948 124989 125011\n",
      "34 0.0 0 250000\n",
      "35 -0.308022848817 124995 125005\n",
      "36 -0.00990625779749 124983 125017\n",
      "37 -0.278460962776 124996 125004\n",
      "38 -0.020148954398 124978 125022\n",
      "39 -0.210223828937 124996 125004\n",
      "40 -0.23761545611 124999 125001\n",
      "41 0.0 0 250000\n",
      "42 0.0 0 250000\n",
      "43 0.0 0 250000\n",
      "44 0.0 0 250000\n",
      "45 0.0 0 250000\n",
      "46 -0.0331691795196 124999 125001\n",
      "47 0.0 0 250000\n",
      "48 0.0 0 250000\n",
      "49 0.0 0 250000\n",
      "50 0.0 0 250000\n",
      "51 0.0 0 250000\n",
      "52 0.0 0 250000\n",
      "53 0.0 0 250000\n",
      "54 0.0 0 250000\n",
      "55 0.0 0 250000\n",
      "56 0.0 0 250000\n",
      "57 0.0 0 250000\n",
      "58 0.0 0 250000\n",
      "59 0.0 0 250000\n",
      "60 -0.0331691795196 124999 125001\n",
      "61 -0.0768266999163 124999 125001\n",
      "62 -0.181979788603 124996 125004\n",
      "63 -0.152549006806 125000 125000\n",
      "64 0.0 0 250000\n",
      "65 0.0 0 250000\n",
      "66 0.0 0 250000\n",
      "67 0.0 0 250000\n",
      "68 -0.295694700715 124999 125001\n",
      "69 -0.186577169085 124943 125057\n",
      "70 -0.190723506948 124989 125011\n",
      "71 0.0 0 250000\n",
      "72 -0.308022848817 124995 125005\n",
      "73 -0.00990625779749 124983 125017\n",
      "74 -0.278460962776 124996 125004\n",
      "75 -0.020148954398 124978 125022\n",
      "76 -0.210223828937 124996 125004\n",
      "77 -0.23761545611 124999 125001\n",
      "78 0.0 0 250000\n",
      "79 0.0 0 250000\n",
      "80 0.0 0 250000\n",
      "81 0.0 0 250000\n",
      "82 0.0 0 250000\n",
      "83 -0.0331691795196 124999 125001\n",
      "84 0.0 0 250000\n",
      "85 -0.0331691795196 124999 125001\n",
      "Dropped 87 equal columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((250000, 61), (250000, 1), (250000, 7))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all = y_loaded.copy()\n",
    "y_all[y_all== -1] = 0\n",
    "\n",
    "x_all, bool_cols = clean_x3(x_loaded)\n",
    "\n",
    "x_all.shape, y_all.shape, bool_cols.shape\n",
    "# 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 313), (250000, 1), (50000, 313), (50000, 1))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose degree\n",
    "degree = 5\n",
    "\n",
    "# build poly from x_all\n",
    "tx_all = build_poly(x_all, degree)\n",
    "# then append the boolean columns\n",
    "tx_all = np.hstack((bool_cols, tx_all))\n",
    "\n",
    "# select a subset\n",
    "subsample = 50000\n",
    "indices = np.random.RandomState(seed = 6).permutation(y_all.shape[0]) # get always the same random array\n",
    "tx_sub, y_sub = tx_all[indices[:subsample]], y_all[indices[:subsample]]\n",
    "\n",
    "tx_all.shape, y_all.shape, tx_sub.shape, y_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Possibily load previously obtained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../miscellanea/best_weights/logistic_regression_44_columns_degree8_keep_all_add_columns/weights\"\n",
    "obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "w = np.array(json.loads(obj_text))\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Choose the degree and set the gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose degree\n",
    "degree = 7\n",
    "tx_sub = build_poly(x_sub, degree)\n",
    "tx_all = build_poly(x_all, degree)\n",
    "tx_sub.shape, y_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-97bb85cf762a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mncolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 50000 data, 14 columns, 999=mean, 0=mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m gamma = np.concatenate([\n\u001b[1;32m      5\u001b[0m      \u001b[0;31m# gamma for constant and 1st degree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_sub' is not defined"
     ]
    }
   ],
   "source": [
    "ncolumns = x_sub.shape[1]\n",
    "\n",
    "# 50000 data, 14 columns, 999=mean, 0=mean\n",
    "gamma = np.concatenate([\n",
    "#      # gamma for constant and 1st degree \n",
    "#     np.ones(ncolumns+1)*1e-5,\n",
    "#     # gammma 2nd degree\n",
    "#     np.ones(ncolumns)*1e-6, \n",
    "#     # gamma for 3rd degree \n",
    "#     np.ones(ncolumns)*1e-7,\n",
    "#     # gamma for 4th degree\n",
    "#     np.ones(ncolumns)*1e-10,\n",
    "#     # gamma for 5th degree\n",
    "#     np.ones(ncolumns)*1e-12,\n",
    "#     # gamma for 6th degree\n",
    "#     np.ones(ncolumns)*1e-15,\n",
    "#     # gamma for 7th degree \n",
    "#     np.ones(ncolumns)*1e-17,\n",
    "#     # gamma for 8th degree \n",
    "#     np.ones(ncolumns)*1e-20,\n",
    "#     # gamma for 9th degree \n",
    "#     np.ones(ncolumns)*1e-24,\n",
    "#     # gamma for 10th degree \n",
    "#     np.ones(ncolumns)*1e-24,\n",
    "#     # gamma for 11th degree \n",
    "#     np.ones(ncolumns)*1e-28,\n",
    "])\\\n",
    ".reshape((-1, 1))*1\n",
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for clean_x3\n",
    "n_bool_cols = bool_cols.shape[1]\n",
    "n_other_cols = x_all.shape[1]\n",
    "\n",
    "# 50000 data, 14 columns, 999=mean, 0=mean\n",
    "gamma = np.concatenate([\n",
    "    # gamma for bool cols\n",
    "    np.ones(n_bool_cols)*1e-5,\n",
    "     # gamma for constant and 1st degree \n",
    "    np.ones(n_other_cols+1)*1e-5,\n",
    "    # gammma 2nd degree\n",
    "    np.ones(n_other_cols)*1e-6, \n",
    "    # gamma for 3rd degree \n",
    "    np.ones(n_other_cols)*1e-7,\n",
    "    # gamma for 4th degree\n",
    "    np.ones(n_other_cols)*1e-9,\n",
    "    # gamma for 5th degree\n",
    "    np.ones(n_other_cols)*1e-13,\n",
    "#     # gamma for 6th degree\n",
    "#     np.ones(n_other_cols)*1e-16,\n",
    "#     # gamma for 7th degree \n",
    "#     np.ones(n_other_cols)*1e-19,\n",
    "#     # gamma for 8th degree \n",
    "#     np.ones(n_other_cols)*1e-20,\n",
    "#     # gamma for 9th degree \n",
    "#     np.ones(n_other_cols)*1e-24,\n",
    "#     # gamma for 10th degree \n",
    "#     np.ones(n_other_cols)*1e-24,\n",
    "#     # gamma for 11th degree \n",
    "#     np.ones(n_other_cols)*1e-28,\n",
    "])\\\n",
    ".reshape((-1, 1))*0.05\n",
    "\n",
    "gamma.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx @ w)\n",
    "    loss = y.T @ (np.log(pred)) + (1 - y).T @ (np.log(1 - pred))\n",
    "#     print((1 - y).T @ (np.log(1 - pred)))\n",
    "    return np.squeeze(- loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(tx @ w)\n",
    "    \n",
    "    grad = tx.T @ (pred - y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    #loss = calculate_loss(y, tx, w) \n",
    "    loss = -1\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w -= gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, tx, gamma_, initial_w = np.array([])):\n",
    "    # init parameters\n",
    "    max_iter = 5000\n",
    "    threshold = 1e-8\n",
    "    gamma = gamma_\n",
    "    losses = []\n",
    "\n",
    "    w = initial_w\n",
    "    if initial_w.size == 0:\n",
    "        w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    highest_ratio = 0\n",
    "    best_w = -1\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        \n",
    "        succ_ratio = compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "        if succ_ratio > highest_ratio: # loss < lowest_loss\n",
    "            #print(1-loss, \"!!\")\n",
    "            highest_ratio = succ_ratio\n",
    "            best_w = w\n",
    "        \n",
    "        if iter % 1000 == 0:\n",
    "            gamma = gamma/2\n",
    "            \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            loss = calculate_loss(y, tx, w) \n",
    "            print(\"Current iteration={i}, loss={l}, prediction={pred}\".format(i=iter, l=loss, pred=succ_ratio))\n",
    "            \n",
    "#         # converge criterion\n",
    "#         losses.append(loss)\n",
    "#         if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "#             break\n",
    "            \n",
    "    succ_ratio = compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "    loss = calculate_loss(y, tx, w) \n",
    "    print(\"Current iteration={i}, loss={l}, prediction={pred}\".format(i=iter, l=loss, pred=succ_ratio))\n",
    "    \n",
    "    # visualization\n",
    "    # visualization(y, x[:, 1:], mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "\n",
    "    return best_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 1), (50000, 200))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sub.shape, tx_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niccolo/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:4: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=nan, prediction=0.820396\n",
      "Current iteration=100, loss=nan, prediction=0.820416\n",
      "Current iteration=200, loss=nan, prediction=0.820424\n",
      "Current iteration=300, loss=nan, prediction=0.820432\n",
      "Current iteration=400, loss=nan, prediction=0.820444\n",
      "Current iteration=500, loss=nan, prediction=0.820436\n",
      "Current iteration=600, loss=nan, prediction=0.820428\n",
      "Current iteration=700, loss=nan, prediction=0.820428\n",
      "Current iteration=800, loss=nan, prediction=0.820444\n",
      "Current iteration=900, loss=nan, prediction=0.820436\n",
      "Current iteration=1000, loss=nan, prediction=0.82044\n",
      "Current iteration=1100, loss=nan, prediction=0.82044\n",
      "Current iteration=1200, loss=nan, prediction=0.820448\n",
      "Current iteration=1300, loss=nan, prediction=0.820464\n",
      "Current iteration=1400, loss=nan, prediction=0.820464\n",
      "Current iteration=1500, loss=nan, prediction=0.820464\n",
      "Current iteration=1600, loss=nan, prediction=0.82046\n",
      "Current iteration=1700, loss=nan, prediction=0.820468\n",
      "Current iteration=1800, loss=nan, prediction=0.82046\n",
      "Current iteration=1900, loss=nan, prediction=0.820456\n",
      "Current iteration=2000, loss=nan, prediction=0.820464\n",
      "Current iteration=2100, loss=nan, prediction=0.820476\n",
      "Current iteration=2200, loss=nan, prediction=0.820476\n",
      "Current iteration=2300, loss=nan, prediction=0.820472\n",
      "Current iteration=2400, loss=nan, prediction=0.820476\n",
      "Current iteration=2500, loss=nan, prediction=0.820472\n",
      "Current iteration=2600, loss=nan, prediction=0.82048\n",
      "Current iteration=2700, loss=nan, prediction=0.820476\n",
      "Current iteration=2800, loss=nan, prediction=0.82048\n",
      "Current iteration=2900, loss=nan, prediction=0.820476\n",
      "Current iteration=3000, loss=nan, prediction=0.820476\n",
      "Current iteration=3100, loss=nan, prediction=0.820476\n",
      "Current iteration=3200, loss=nan, prediction=0.820472\n",
      "Current iteration=3300, loss=nan, prediction=0.820468\n",
      "Current iteration=3400, loss=nan, prediction=0.820472\n",
      "Current iteration=3500, loss=nan, prediction=0.820472\n",
      "Current iteration=3600, loss=nan, prediction=0.820464\n",
      "Current iteration=3700, loss=nan, prediction=0.820468\n",
      "Current iteration=3800, loss=nan, prediction=0.820476\n",
      "Current iteration=3900, loss=nan, prediction=0.82048\n",
      "Current iteration=4000, loss=nan, prediction=0.820484\n",
      "Current iteration=4100, loss=nan, prediction=0.820488\n",
      "Current iteration=4200, loss=nan, prediction=0.820492\n",
      "Current iteration=4300, loss=nan, prediction=0.820492\n",
      "Current iteration=4400, loss=nan, prediction=0.820488\n",
      "Current iteration=4500, loss=nan, prediction=0.820492\n",
      "Current iteration=4600, loss=nan, prediction=0.820492\n",
      "Current iteration=4700, loss=nan, prediction=0.820492\n",
      "Current iteration=4800, loss=nan, prediction=0.820488\n",
      "Current iteration=4900, loss=nan, prediction=0.820488\n",
      "Current iteration=4999, loss=nan, prediction=0.820492\n"
     ]
    }
   ],
   "source": [
    "# w, s_te, s_tr \n",
    "# for i in range(100):\n",
    "w = logistic_regression_gradient_descent_demo(y_sub, tx_sub, gamma, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.820492"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_batch(y, tx, gamma_, initial_w = np.array([])):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = gamma_\n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    w = initial_w\n",
    "    if initial_w.size == 0:\n",
    "        w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    lowest_loss = float('Inf')\n",
    "    best_w = -1\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        k_curr = iter % k_sets\n",
    "        from_ = k_curr*50000\n",
    "        to_ = (k_curr+1)*50000\n",
    "        \n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y[from_:to_], tx[from_:to_], w, gamma)\n",
    "        \n",
    "\n",
    "#         succ_ratio = 1 - compute_loss(y_correct, tx, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "        if loss < lowest_loss: # loss < lowest_loss\n",
    "            lowest_loss = loss # loss\n",
    "            best_w = w\n",
    "            \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            succ_ratio_test = compute_loss(y, tx, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "            print(\"Current iteration={i}, loss={l}, prediction={pred}\".format(i=iter, l=loss, pred=succ_ratio_test))\n",
    "            \n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    succ_ratio_test = compute_loss(y, tx, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "    print(\"Current iteration={i}, loss={l}, prediction={pred}\".format(i=iter, l=loss, pred=succ_ratio_test))\n",
    "\n",
    "    return best_w #, succ_ratio_test, succ_ratio_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = logistic_regression_penalized_gradient_descent_demo(y_sub, tx_sub, gamma)# w)\n",
    "w = log_reg_batch(y_all, tx_all, gamma, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the current found weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"weigths\"\n",
    "json.dump(w.tolist(), codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\"\"\"\n",
    "    num_samples = y.shape[0]\n",
    "    #loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    loss = -1\n",
    "    gradient = calculate_gradient(y, tx, w) + 2 * lambda_ * w\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_pen_batch(y, tx, gamma_, initial_w = np.array([])):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    lambda_ = 1\n",
    "    threshold = 1e-18\n",
    "    \n",
    "    gamma = gamma_\n",
    "    \n",
    "#     seed = 2\n",
    "#     k_sets = 5\n",
    "#     k_indices = build_k_indices_(y, k_sets)\n",
    "#     train=[]\n",
    "#     test=[]\n",
    "#     for i in range(k_sets):\n",
    "#         tr, te = get_kth_set(y_all, tx_all, k_indices, i)\n",
    "#         train.append(tr)\n",
    "#         test.append(te)\n",
    "    \n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    if initial_w.size == 0:\n",
    "        w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    highest_ratio = 0\n",
    "    best_w = -1\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "#         # at each iteration take the next set\n",
    "#         shuffle_indices = np.random.permutation(np.arange(y.shape[0]))\n",
    "#         y_train = y[shuffle_indices[:50000]]\n",
    "#         tx_train = tx[shuffle_indices[:50000]]\n",
    "\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y_train, tx_train, w, gamma, lambda_) # use test set which is smaller\n",
    "        \n",
    "        succ_ratio = compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "        if succ_ratio > highest_ratio: # loss < lowest_loss\n",
    "            #print(1-loss, \"!!\")\n",
    "            highest_ratio = succ_ratio\n",
    "            best_w = w\n",
    "            \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            loss = calculate_loss(y, tx, w) \n",
    "            succ_ratio = compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "            print(\"Current iteration={i}, loss={l}, prediction={pred}\".format(i=iter, l=loss, pred=succ_ratio))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    succ_ratio = compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "    loss = calculate_loss(y, tx, w) \n",
    "    print(\"Current iteration={i}, loss={l}, prediction={pred}\".format(i=iter, l=loss, pred=succ_ratio))\n",
    "    \n",
    "    return best_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, tx, gamma_, initial_w = np.array([])):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    lambda_ = 1\n",
    "    threshold = 1e-18\n",
    "    \n",
    "    gamma = gamma_\n",
    "    succ_ratio_test = -1\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    if initial_w.size == 0:\n",
    "        w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    lowest_loss = float('Inf')\n",
    "    best_w = w\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        \n",
    "        loss = 1-compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "        if loss < lowest_loss:\n",
    "            print(1-loss, \"!!\")\n",
    "            lowest_loss = loss\n",
    "            best_w = w\n",
    "            \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}, prediction={pred}\".format(i=iter, l=loss, pred=succ_ratio_test))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "#         if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "#             break\n",
    "\n",
    "    loss = 1-compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)\n",
    "    print(\"Current iteration={i}, loss={l}, prediction={pred}\".format(i=iter, l=1-loss, pred=succ_ratio_test))\n",
    "    \n",
    "    return best_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = logistic_regression_penalized_gradient_descent_demo(y_sub, tx_sub, gamma, w)\n",
    "w = logistic_regression_penalized_gradient_descent_demo(y_sub, tx_sub, gamma, w)\n",
    "# w = log_reg_pen_batch(y_all, tx_all, gamma)# w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w_curr_best.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_curr_best = w.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.820492"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(y_all, tx_all, w, costfunc=CostFunction.SUCCESS_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create submit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((568238,), (568238, 30))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test dataset\n",
    "data_path = \"../dataset/test.csv\"\n",
    "y_te_loaded, x_te_loaded, ids_te = load_csv_data(data_path, sub_sample=False)\n",
    "y_te_loaded.shape, x_te_loaded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clean_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean in the same way of the trained set\n",
    "# x_te, kept_cols = clean_x(x_te_loaded, corr, subs_func=np.nanmean, bool_col=True)\n",
    "x_te = clean_x2(x_te_loaded)\n",
    "x_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the poly\n",
    "# degree = 8\n",
    "tx_te = build_poly(x_te, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean_x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 boolean columns have been created.\n",
      "6 columns have been removed:  [9, 15, 18, 20, 22, 23]\n",
      "Dropped 87 equal columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((568238, 61), (568238, 7))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te, bool_cols_te = clean_x3(x_te_loaded)\n",
    "x_te.shape, bool_cols_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the poly\n",
    "tx_te = build_poly(x_te, degree) \n",
    "tx_te = np.hstack((bool_cols_te, tx_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and create file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((568238, 1), 314794, 253444)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "y_te_pred = predict_labels(w, tx_te)\n",
    "y_te_pred.shape, (y_te_pred==-1).sum(), (y_te_pred==1).sum()\n",
    "# ((568238, 1), 391856, 176382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove also the columns with \"equal\" distribution\n",
    "# to_be_removed = np.where(np.isin(kept_cols, [\"PRI_tau_phi\", \"PRI_lep_phi\", \"PRI_met_phi\"])) # PRI_jet_num\n",
    "# x_te = np.delete(x_te, to_be_removed, axis=1)\n",
    "# x_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predictions\n",
    "create_csv_submission(ids_te, y_te_pred, \"logistic_regression_clean_x3_degree5\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
