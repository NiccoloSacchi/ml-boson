{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from implementations import * \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to test the functions\n",
    "from helpers import *\n",
    "\n",
    "x, y, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(x)\n",
    "tx = build_poly(x, 1)\n",
    "\n",
    "y.shape, x.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "initial_w = np.array([0, 0])\n",
    "max_iters = 20\n",
    "gamma = 0.5\n",
    "loss, w = gradient_descent(y, tx, initial_w, max_iters, gamma, print_output=False, plot_losses=True)\n",
    "loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent\n",
    "# gradient descent\n",
    "initial_w = np.array([0, 0])\n",
    "max_iters = 20\n",
    "gamma = 0.5\n",
    "loss, w = gradient_descent(y, tx, initial_w, max_iters, gamma, batch_size=1, print_output=False, plot_losses=True)\n",
    "loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analityc solution\n",
    "least_squares(y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression to choose smaller weights (the simpler the model the better)\n",
    "lambda_ = 0.02 # how to properly choose lambda?\n",
    "ridge_regression(y, tx, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression (it is gradient descent with a different loss function)\n",
    "from helpers import sample_data, load_data\n",
    "from plots import visualization\n",
    "\n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "x, mean_x, std_x = standardize(X)\n",
    "\n",
    "tx = build_poly(x, 1)\n",
    "initial_w = np.ones(tx.shape[1])\n",
    "max_iters = 50\n",
    "gamma = 0.2\n",
    "minloss, w = gradient_descent(y, tx, initial_w, max_iters, gamma, batch_size=-1, print_output=False, plot_losses=True, costfunc=CostFunction.PROB)\n",
    "# w = np.array(w).reshape(-1, 1)\n",
    "visualization(y, x, mean_x, std_x, w, \"classification_by_least_square\")\n",
    "minloss, w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
