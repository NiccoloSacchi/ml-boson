{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import codecs, json \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_tr removing all samples with -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to test the functions\n",
    "from helpers import *\n",
    "\n",
    "ids_tr,predictions_tr,data_tr, ids_te,data_te = load_boson_data()\n",
    "\n",
    "invalid_indexes= np.unique(np.where(data_tr == -999)[0])  \n",
    "invalid_indexes   #\"indexes\" of the rows where -999 is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_tr = np.delete(data_tr,np.unique(np.where(data_tr == -999)[0]),0)\n",
    "\n",
    "clean_data_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_tr replacing -999 with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with_zero_data_tr = np.where(data_tr == -999, 0, data_tr)\n",
    "\n",
    "with_zero_data_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_tr replacing -999 with the mean/median/mfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some analysys with the help of pandas and seaborn\n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "ndata = 100\n",
    "data_path = \"../dataset/train.csv\"\n",
    "_, data_loaded, _ = load_partial_csv_data(data_path, ndata)\n",
    "print(data_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = data_loaded\n",
    "# x_, ndropped = drop_corr_columns(data_loaded, 0.5)\n",
    "x_ = pd.DataFrame(x_)\n",
    "x_.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of features and pairwise relationship using seaborn pairplot\n",
    "sbn.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print the pairwise correlation between the columns in data frame\n",
    "\n",
    "correlations = data.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that have a strong correlation with other fatures:\n",
    "* DER_tp_h\n",
    "* DER_deltaeta_jet_jet\n",
    "* DER_mass_jet_jet\n",
    "* DER_prodeta_jet_jet\n",
    "* DER_sum_pt\n",
    "* DER_met_phi_centrality\n",
    "* DER_lep_eta_centrality\n",
    "* PRI_met_sumet\n",
    "* PRI_jet_num\n",
    "* PRI_jet_leading_pt\n",
    "* PRI_jet_leading_eta\n",
    "* PRI_jet_leading_phi\n",
    "* PRI_jet_subleading_pt\n",
    "* PRI_jet_subleading_eta\n",
    "* PRI_jet_subleading_phi\n",
    "* PRI_jet_all_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "data_path = \"../dataset/train.csv\"\n",
    "y_loaded, data_loaded, _ = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "data_path = \"../dataset/test.csv\"\n",
    "_, data_test_loaded, _ = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 30), (568238, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaded.shape, data_test_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged= np.concatenate((data_loaded,data_test_loaded), axis=0)\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN DATA: YOU MAY WANT TO CHAGE THIS STEP\n",
    "data = drop_columns_with_70_nan_ratio(data_merged)\n",
    "data = fill_with_nan_list(data, nan_values=[0, -999])\n",
    "data = sustitute_nans(data, substitutions=np.nanmean(data, axis=0))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE AND STORE CORRELATION MATRIX\n",
    "corr_matrix = np.abs(np.corrcoef(data.T)) \n",
    "file_path = \"corr_matrix23.json\"\n",
    "json.dump(corr_matrix.tolist(), codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: LOAD CORRELATION MATRIX\n",
    "file_path = \"corr_matrix23.json\"\n",
    "obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "b_new = json.loads(obj_text)\n",
    "corr_matrix_loaded = np.array(b_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exaple: GET CORRELATED COLUMNS\n",
    "corr_matrix = np.abs(corr_matrix) > 0.6\n",
    "# remove i from the list (i is surely correlated to himself)\n",
    "for i in range(23):\n",
    "    corr_matrix[i][i] = False\n",
    "    \n",
    "correlated_to = {}\n",
    "for i in range(23):\n",
    "    c = np.where(corr_matrix[i])[0]\n",
    "    if len(c) > 0: # if it is not correlated to any other column then ignore it\n",
    "        correlated_to[i] = c\n",
    "correlated_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ditributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data as it is\n",
    "plot_distributions(data_loaded, y_loaded, col_labels = column_labels(), title = \"distribution_rawData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data after cleaning it is\n",
    "x_cleaned, keptCols = clean_x(data_loaded, 0.7, subs_func = np.nanmean)\n",
    "\n",
    "plot_distributions(x_cleaned, y_loaded, col_labels = keptCols, title = \"distribution_cleanedData_999=mean_0=mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data after cleaning it is\n",
    "x_cleaned, keptCols = clean_x(data_loaded, 0.7, subs_func = None)\n",
    "\n",
    "plot_distributions(x_cleaned, y_loaded, col_labels = keptCols, title = \"distribution_cleanedData_999=999_0=0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better norm the hist to have a clearer idea of the proportions\n",
    "f, ax = plt.subplots(2, 2)\n",
    "dataset1 = [0, 0, 0, 0, 1, 2, 4, 6]\n",
    "dataset2 = [0, 0, 1, 6]\n",
    "\n",
    "ax[0][0].hist(dataset1, histtype='step',color=\"red\", normed=True)\n",
    "ax[0][1].hist(dataset1, histtype='step',color=\"red\", normed=False)\n",
    "\n",
    "\n",
    "ax[1][0].hist(dataset2, histtype='step', color=\"blue\", normed=True)\n",
    "ax[1][1].hist(dataset2, histtype='step', color=\"blue\", normed=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -999 values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are three groups of columns with the -999 values in the same positions\n",
    "print(np.sum(x_loaded == -999, axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(data_loaded)\n",
    "\n",
    "pri = np.where(column_labels() == \"PRI_jet_num\")\n",
    "dropped = list(set(range(30))- set([0,  1,  2,  3,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 29]))\n",
    "d = data[np.any(data==-999, axis=1)].loc[:, np.append(dropped ,pri)]\n",
    "\n",
    "d[d[22] <= 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d[22] > 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(data_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818238, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = fill_with_nan_list(data_merged, nan_values=[0, -999])\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 101, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 30)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]\n",
    "scan_perc = list(range(0, 101, 5))\n",
    "col_perc = np.zeros((len(scan_perc), data_merged.shape[1]))\n",
    "for col in range(data_merged.shape[1]):\n",
    "    for i, p in enumerate(scan_perc):\n",
    "         col_perc[i, col] = np.nanpercentile(data_merged[:, col], p)\n",
    "col_perc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3.416 ,    6.9598,   10.717 ,   14.79  ,   19.304 ,   24.154 ,\n",
       "         29.393 ,   34.912 ,   40.635 ,   46.484 ,   52.3914,   58.124 ,\n",
       "         63.581 ,   68.721 ,   73.62  ,   78.574 ,   84.19  ,   91.645 ,\n",
       "        104.529 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_perc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store both the computed percentiles and the list of % to which they correspond\n",
    "file_path = \"percentiles.json\"\n",
    "map_ = {\n",
    "    \"scan_perc\": scan_perc,\n",
    "    \"col_perc\": col_perc.tolist()\n",
    "}\n",
    "json.dump(map_, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
