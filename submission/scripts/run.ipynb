{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, check the paths to the train file and to the test file are correct:\n",
      "../../dataset/train.csv\n",
      "../../dataset/test.csv\n",
      "Loading train data...\n",
      "Train data loaded\n",
      "Capping the outliers\n",
      "Jet 0 useless columns dropped: [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "Jet 1 useless columns dropped: [4, 5, 6, 12, 22, 26, 27, 28]\n",
      "Jet 2 useless columns dropped: [22]\n",
      "Jet 3 useless columns dropped: [22]\n",
      "Train data cleaned\n",
      "The train polynomials have been built\n",
      "Starting to run the logistic regression:\n",
      "-Logistic regression on dataset 0/3\n",
      "Gradient Descent(0/1999): loss=64268.2203846304, prediction ratio=0.7449280874360694\n",
      "Gradient Descent(50/1999): loss=35844.15651465327, prediction ratio=0.8429033258935273\n",
      "Gradient Descent(100/1999): loss=35324.298035468426, prediction ratio=0.8453054157116692\n",
      "Gradient Descent(150/1999): loss=35109.24124066348, prediction ratio=0.8456256943540881\n",
      "Gradient Descent(200/1999): loss=34984.4930347388, prediction ratio=0.84616616456317\n",
      "Gradient Descent(250/1999): loss=34898.75450065154, prediction ratio=0.8465164693283156\n",
      "Gradient Descent(300/1999): loss=34834.50430428537, prediction ratio=0.8466465825267984\n",
      "Gradient Descent(350/1999): loss=34783.95411017786, prediction ratio=0.847116991782851\n",
      "Gradient Descent(400/1999): loss=34742.903799113294, prediction ratio=0.8473672094722409\n",
      "Gradient Descent(450/1999): loss=34708.78232010882, prediction ratio=0.847387226887392\n",
      "Gradient Descent(500/1999): loss=34679.89399689803, prediction ratio=0.8473772181798165\n",
      "Gradient Descent(550/1999): loss=34655.06540378263, prediction ratio=0.8473672094722409\n",
      "Gradient Descent(600/1999): loss=34633.45446978409, prediction ratio=0.8473171659343629\n",
      "Gradient Descent(650/1999): loss=34614.43894677448, prediction ratio=0.8472771311040606\n",
      "Gradient Descent(700/1999): loss=34597.547889978654, prediction ratio=0.8473672094722409\n",
      "Gradient Descent(750/1999): loss=34582.41777548761, prediction ratio=0.8473572007646653\n",
      "Gradient Descent(800/1999): loss=34568.76335138099, prediction ratio=0.84743727042527\n",
      "Gradient Descent(850/1999): loss=34556.35765321322, prediction ratio=0.8474873139631479\n",
      "Gradient Descent(900/1999): loss=34545.01792629409, prediction ratio=0.8474873139631479\n",
      "Gradient Descent(950/1999): loss=34534.595476447386, prediction ratio=0.8474773052555723\n",
      "Gradient Descent(1000/1999): loss=34524.96820581515, prediction ratio=0.8475173400858748\n",
      "Gradient Descent(1050/1999): loss=34516.03502794889, prediction ratio=0.8475673836237526\n",
      "Gradient Descent(1100/1999): loss=34507.71162584657, prediction ratio=0.8476674706995085\n",
      "Gradient Descent(1150/1999): loss=34499.92718738764, prediction ratio=0.8476374445767818\n",
      "Gradient Descent(1200/1999): loss=34492.62186374387, prediction ratio=0.8477375316525377\n",
      "Gradient Descent(1250/1999): loss=34485.744770333564, prediction ratio=0.8477275229449621\n",
      "Gradient Descent(1300/1999): loss=34479.252400183555, prediction ratio=0.8476474532843574\n",
      "Gradient Descent(1350/1999): loss=34473.107354398126, prediction ratio=0.8476974968222354\n",
      "Gradient Descent(1400/1999): loss=34467.27731897548, prediction ratio=0.8476974968222354\n",
      "Gradient Descent(1450/1999): loss=34461.7342347736, prediction ratio=0.8477175142373865\n",
      "Gradient Descent(1500/1999): loss=34456.45362017323, prediction ratio=0.8478476274358692\n",
      "Gradient Descent(1550/1999): loss=34451.4140153613, prediction ratio=0.8478176013131424\n",
      "Gradient Descent(1600/1999): loss=34446.5965241348, prediction ratio=0.8479176883888984\n",
      "Gradient Descent(1650/1999): loss=34441.98443437835, prediction ratio=0.8479777406343519\n",
      "Gradient Descent(1700/1999): loss=34437.562902361235, prediction ratio=0.8479677319267763\n",
      "Gradient Descent(1750/1999): loss=34433.31868906588, prediction ratio=0.8479777406343519\n",
      "Gradient Descent(1800/1999): loss=34429.23993913201, prediction ratio=0.8480277841722298\n",
      "Gradient Descent(1850/1999): loss=34425.31599485307, prediction ratio=0.8479477145116251\n",
      "Gradient Descent(1900/1999): loss=34421.537239116835, prediction ratio=0.8479677319267763\n",
      "Gradient Descent(1950/1999): loss=34417.894962334074, prediction ratio=0.8480077667570787\n",
      "Gradient Descent(1999/1999): loss=34414.45031139816, prediction ratio=0.8479977580495031\n",
      "-Logistic regression on dataset 1/3\n",
      "Gradient Descent(0/1999): loss=48172.04264789934, prediction ratio=0.6515784586815228\n",
      "Gradient Descent(50/1999): loss=33447.06945541494, prediction ratio=0.8035051067780873\n",
      "Gradient Descent(100/1999): loss=32964.615349494954, prediction ratio=0.8062261425771176\n",
      "Gradient Descent(150/1999): loss=32771.93805730176, prediction ratio=0.8065872278964201\n",
      "Gradient Descent(200/1999): loss=32661.468499258182, prediction ratio=0.8069096255029403\n",
      "Gradient Descent(250/1999): loss=32584.93410190486, prediction ratio=0.8074383575776334\n",
      "Gradient Descent(300/1999): loss=32526.53440644649, prediction ratio=0.80786392241824\n",
      "Gradient Descent(350/1999): loss=32479.54851104724, prediction ratio=0.8084442381099762\n",
      "Gradient Descent(400/1999): loss=32440.497086514843, prediction ratio=0.8086505725781492\n",
      "Gradient Descent(450/1999): loss=32407.3130230261, prediction ratio=0.8089858660889302\n",
      "Gradient Descent(500/1999): loss=32378.64896500131, prediction ratio=0.8093082636954503\n",
      "Gradient Descent(550/1999): loss=32353.57002752555, prediction ratio=0.8094888063551016\n",
      "Gradient Descent(600/1999): loss=32331.398425259023, prediction ratio=0.8095919735891881\n",
      "Gradient Descent(650/1999): loss=32311.62648519091, prediction ratio=0.8097467244403177\n",
      "Gradient Descent(700/1999): loss=32293.864099189697, prediction ratio=0.810198081089446\n",
      "Gradient Descent(750/1999): loss=32277.805159813757, prediction ratio=0.8102367688022284\n",
      "Gradient Descent(800/1999): loss=32263.20520878878, prediction ratio=0.8103657278448365\n",
      "Gradient Descent(850/1999): loss=32249.86606096879, prediction ratio=0.8105075827917053\n",
      "Gradient Descent(900/1999): loss=32237.624942162452, prediction ratio=0.8104946868874445\n",
      "Gradient Descent(950/1999): loss=32226.34663999071, prediction ratio=0.8105591664087486\n",
      "Gradient Descent(1000/1999): loss=32215.917718146065, prediction ratio=0.810597854121531\n",
      "Gradient Descent(1050/1999): loss=32206.242175231506, prediction ratio=0.8105462705044878\n",
      "Gradient Descent(1100/1999): loss=32197.238134951534, prediction ratio=0.8104946868874445\n",
      "Gradient Descent(1150/1999): loss=32188.83528581664, prediction ratio=0.8104173114618797\n",
      "Gradient Descent(1200/1999): loss=32180.972874462757, prediction ratio=0.8105075827917053\n",
      "Gradient Descent(1250/1999): loss=32173.59811404535, prediction ratio=0.8105075827917053\n",
      "Gradient Descent(1300/1999): loss=32166.66490814986, prediction ratio=0.8105849582172702\n",
      "Gradient Descent(1350/1999): loss=32160.132817600006, prediction ratio=0.8107010213556175\n",
      "Gradient Descent(1400/1999): loss=32153.96621645354, prediction ratio=0.8107139172598783\n",
      "Gradient Descent(1450/1999): loss=32148.133596942793, prediction ratio=0.8107268131641391\n",
      "Gradient Descent(1500/1999): loss=32142.606992841385, prediction ratio=0.8107139172598783\n",
      "Gradient Descent(1550/1999): loss=32137.361497851856, prediction ratio=0.8108041885897039\n",
      "Gradient Descent(1600/1999): loss=32132.374860875694, prediction ratio=0.8108299803982255\n",
      "Gradient Descent(1650/1999): loss=32127.627143971353, prediction ratio=0.8108041885897039\n",
      "Gradient Descent(1700/1999): loss=32123.100431791798, prediction ratio=0.8108299803982255\n",
      "Gradient Descent(1750/1999): loss=32118.77858357568, prediction ratio=0.8108557722067471\n",
      "Gradient Descent(1800/1999): loss=32114.64702052832, prediction ratio=0.8109073558237904\n",
      "Gradient Descent(1850/1999): loss=32110.69254280061, prediction ratio=0.8109202517280512\n",
      "Gradient Descent(1900/1999): loss=32106.903171350878, prediction ratio=0.8109847312493552\n",
      "Gradient Descent(1950/1999): loss=32103.26801082724, prediction ratio=0.8110105230578768\n",
      "Gradient Descent(1999/1999): loss=32099.845593948536, prediction ratio=0.8109589394408336\n",
      "-Logistic regression on dataset 2/3\n",
      "Gradient Descent(0/1999): loss=26246.698346836874, prediction ratio=0.760634391313841\n",
      "Gradient Descent(50/1999): loss=19657.053243555576, prediction ratio=0.8266341134202744\n",
      "Gradient Descent(100/1999): loss=19152.645373722073, prediction ratio=0.8305841719764188\n",
      "Gradient Descent(150/1999): loss=18910.4037845953, prediction ratio=0.8327477718890808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(200/1999): loss=18774.960910800448, prediction ratio=0.8341372397229004\n",
      "Gradient Descent(250/1999): loss=18689.362897836385, prediction ratio=0.834951070882709\n",
      "Gradient Descent(300/1999): loss=18631.055411254645, prediction ratio=0.8352488139899561\n",
      "Gradient Descent(350/1999): loss=18589.040887528543, prediction ratio=0.8357847515830008\n",
      "Gradient Descent(400/1999): loss=18557.297763666327, prediction ratio=0.8360427956092816\n",
      "Gradient Descent(450/1999): loss=18532.31625697155, prediction ratio=0.8363008396355625\n",
      "Gradient Descent(500/1999): loss=18511.95948566297, prediction ratio=0.8364794854999107\n",
      "Gradient Descent(550/1999): loss=18494.87994962761, prediction ratio=0.8365985827428095\n",
      "Gradient Descent(600/1999): loss=18480.200783776116, prediction ratio=0.836955874471506\n",
      "Gradient Descent(650/1999): loss=18467.334310653914, prediction ratio=0.8367970781476409\n",
      "Gradient Descent(700/1999): loss=18455.8755467097, prediction ratio=0.8370352726334386\n",
      "Gradient Descent(750/1999): loss=18445.537995467246, prediction ratio=0.8371345203358542\n",
      "Gradient Descent(800/1999): loss=18436.11392990226, prediction ratio=0.837114670795371\n",
      "Gradient Descent(850/1999): loss=18427.44921257506, prediction ratio=0.8372139184977868\n",
      "Gradient Descent(900/1999): loss=18419.426959493227, prediction ratio=0.8373330157406856\n",
      "Gradient Descent(950/1999): loss=18411.95671247515, prediction ratio=0.8372933166597193\n",
      "Gradient Descent(1000/1999): loss=18404.967119537177, prediction ratio=0.8373528652811687\n",
      "Gradient Descent(1050/1999): loss=18398.400895410938, prediction ratio=0.8374322634431013\n",
      "Gradient Descent(1100/1999): loss=18392.21129202265, prediction ratio=0.8373131662002025\n",
      "Gradient Descent(1150/1999): loss=18386.35958618347, prediction ratio=0.8372139184977868\n",
      "Gradient Descent(1200/1999): loss=18380.81326349565, prediction ratio=0.8372337680382699\n",
      "Gradient Descent(1250/1999): loss=18375.544685909103, prediction ratio=0.8371742194168205\n",
      "Gradient Descent(1300/1999): loss=18370.530100031145, prediction ratio=0.8370352726334386\n",
      "Gradient Descent(1350/1999): loss=18365.7488887745, prediction ratio=0.8370154230929554\n",
      "Gradient Descent(1400/1999): loss=18361.182999055283, prediction ratio=0.8370749717144048\n",
      "Gradient Descent(1450/1999): loss=18356.81649847613, prediction ratio=0.837094821254888\n",
      "Gradient Descent(1500/1999): loss=18352.635227677034, prediction ratio=0.8371742194168205\n",
      "Gradient Descent(1550/1999): loss=18348.626524491207, prediction ratio=0.8372337680382699\n",
      "Gradient Descent(1600/1999): loss=18344.779002620933, prediction ratio=0.8371940689573036\n",
      "Gradient Descent(1650/1999): loss=18341.08237217368, prediction ratio=0.8371742194168205\n",
      "Gradient Descent(1700/1999): loss=18337.527292686013, prediction ratio=0.837114670795371\n",
      "Gradient Descent(1750/1999): loss=18334.105251623852, prediction ratio=0.8372734671192362\n",
      "Gradient Descent(1800/1999): loss=18330.808463059613, prediction ratio=0.8372139184977868\n",
      "Gradient Descent(1850/1999): loss=18327.62978248128, prediction ratio=0.8372337680382699\n",
      "Gradient Descent(1900/1999): loss=18324.562634616184, prediction ratio=0.8372933166597193\n",
      "Gradient Descent(1950/1999): loss=18321.600951844794, prediction ratio=0.8373131662002025\n",
      "Gradient Descent(1999/1999): loss=18318.795413653075, prediction ratio=0.8373330157406856\n",
      "-Logistic regression on dataset 3/3\n",
      "Gradient Descent(0/1999): loss=13784.451906937436, prediction ratio=0.6963093304457679\n",
      "Gradient Descent(50/1999): loss=8971.261693182689, prediction ratio=0.8285056848944234\n",
      "Gradient Descent(100/1999): loss=8737.171137325187, prediction ratio=0.8326114419779823\n",
      "Gradient Descent(150/1999): loss=8622.770312029443, prediction ratio=0.8344161703663598\n",
      "Gradient Descent(200/1999): loss=8550.037084506042, prediction ratio=0.836491608012994\n",
      "Gradient Descent(250/1999): loss=8499.22360734549, prediction ratio=0.8372134993683451\n",
      "Gradient Descent(300/1999): loss=8461.77750795363, prediction ratio=0.8378000360945678\n",
      "Gradient Descent(350/1999): loss=8433.013293031943, prediction ratio=0.8378000360945678\n",
      "Gradient Descent(400/1999): loss=8410.197935005985, prediction ratio=0.8383865728207904\n",
      "Gradient Descent(450/1999): loss=8391.626838502227, prediction ratio=0.8382060999819527\n",
      "Gradient Descent(500/1999): loss=8376.181728123158, prediction ratio=0.8382512181916622\n",
      "Gradient Descent(550/1999): loss=8363.099955528982, prediction ratio=0.8384316910304999\n",
      "Gradient Descent(600/1999): loss=8351.845113370418, prediction ratio=0.8384316910304999\n",
      "Gradient Descent(650/1999): loss=8342.030131388705, prediction ratio=0.8383865728207904\n",
      "Gradient Descent(700/1999): loss=8333.369468292927, prediction ratio=0.8382512181916622\n",
      "Gradient Descent(750/1999): loss=8325.648337567627, prediction ratio=0.8388377549178848\n",
      "Gradient Descent(800/1999): loss=8318.702335111378, prediction ratio=0.8391987005955603\n",
      "Gradient Descent(850/1999): loss=8312.40363087117, prediction ratio=0.8391084641761415\n",
      "Gradient Descent(900/1999): loss=8306.651413829833, prediction ratio=0.8391535823858509\n",
      "Gradient Descent(950/1999): loss=8301.365155362144, prediction ratio=0.8395145280635264\n",
      "Gradient Descent(1000/1999): loss=8296.479777170352, prediction ratio=0.8396498826926547\n",
      "Gradient Descent(1050/1999): loss=8291.942129481067, prediction ratio=0.8395145280635264\n",
      "Gradient Descent(1100/1999): loss=8287.708385723656, prediction ratio=0.839469409853817\n",
      "Gradient Descent(1150/1999): loss=8283.742088337667, prediction ratio=0.8393340552246887\n",
      "Gradient Descent(1200/1999): loss=8280.01266405776, prediction ratio=0.8397401191120736\n",
      "Gradient Descent(1250/1999): loss=8276.49428245069, prediction ratio=0.8399657101606208\n",
      "Gradient Descent(1300/1999): loss=8273.16496873091, prediction ratio=0.8399205919509114\n",
      "Gradient Descent(1350/1999): loss=8270.00590727478, prediction ratio=0.8401461829994585\n",
      "Gradient Descent(1400/1999): loss=8267.00088980067, prediction ratio=0.8401461829994585\n",
      "Gradient Descent(1450/1999): loss=8264.135874468324, prediction ratio=0.8400559465800397\n",
      "Gradient Descent(1500/1999): loss=8261.398630863967, prediction ratio=0.8401461829994585\n",
      "Gradient Descent(1550/1999): loss=8258.778452093457, prediction ratio=0.8402364194188775\n",
      "Gradient Descent(1600/1999): loss=8256.265919750873, prediction ratio=0.8401461829994585\n",
      "Gradient Descent(1650/1999): loss=8253.85271086997, prediction ratio=0.8404168922577152\n",
      "Gradient Descent(1700/1999): loss=8251.53143844684, prediction ratio=0.8404168922577152\n",
      "Gradient Descent(1750/1999): loss=8249.295518983796, prediction ratio=0.8404168922577152\n",
      "Gradient Descent(1800/1999): loss=8247.13906191492, prediction ratio=0.8403266558382964\n",
      "Gradient Descent(1850/1999): loss=8245.056776851785, prediction ratio=0.8405071286771341\n",
      "Gradient Descent(1900/1999): loss=8243.043895418685, prediction ratio=0.8405071286771341\n",
      "Gradient Descent(1950/1999): loss=8241.096105092147, prediction ratio=0.8405522468868435\n",
      "Gradient Descent(1999/1999): loss=8239.191975633115, prediction ratio=0.8401461829994585\n",
      "Loading test data...\n",
      "Test data loaded\n",
      "Capping the outliers\n",
      "Jet 0 useless columns dropped: [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "Jet 1 useless columns dropped: [4, 5, 6, 12, 22, 26, 27, 28]\n",
      "Jet 2 useless columns dropped: [22]\n",
      "Jet 3 useless columns dropped: [22]\n",
      "Test data cleaned.\n",
      "The test polynomials have been built\n",
      "I predicted  387628 -1s and  180610 1s\n",
      "Prediction store in filepredictions\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from proj1_helpers import load_csv_data, create_csv_submission, predict_labels\n",
    "import numpy as np\n",
    "from cleaner import clean_input_data, concatenate_log\n",
    "from implementations import build_poly, logistic_regression, compute_loss\n",
    "\n",
    "# (CHECK THE PATHs ARE CORRECT)\n",
    "train_data_path = \"../../dataset/train.csv\"\n",
    "test_data_path = \"../../dataset/test.csv\"\n",
    "print(\"Please, check the paths to the train file and to the test file are correct:\")\n",
    "print(train_data_path)\n",
    "print(test_data_path)\n",
    "\n",
    "# 1. load the train data \n",
    "print(\"Loading train data...\")\n",
    "y_loaded, x_loaded, ids_te = load_csv_data(train_data_path, sub_sample=False)\n",
    "y_loaded = y_loaded.reshape((-1, 1))\n",
    "print(\"Train data loaded\")\n",
    "\n",
    "# 2. clean the train data and concatenate to each dataset its log\n",
    "xs, ys = clean_input_data(x_loaded.copy(), y_loaded.copy(), corr=1, dimension_expansion=5, bool_col=True)\n",
    "for jet in range(4): # set -1 to 0 \n",
    "    ys[jet][ys[jet]== -1] = 0 \n",
    "    \n",
    "xs, mean_log, std_log = concatenate_log(xs.copy())\n",
    "print(\"Train data cleaned\")\n",
    "\n",
    "# 3. Build the polynomials (one for each one of the 4 datasets)\n",
    "degree = 2\n",
    "txs = [None]*4\n",
    "for jet in range(4):\n",
    "    txs[jet] = build_poly(xs[jet], degree)\n",
    "print(\"The train polynomials have been built\")\n",
    "    \n",
    "# 4. Set the array of gammas for the logistic regression\n",
    "gamma_constants = [1e-5, 1e-6] # one ofr the degree 1 and one for the degree 2\n",
    "gammas = [None]*4\n",
    "for jet in range(4):\n",
    "    ncolumns = xs[jet].shape[1]\n",
    "    gammas[jet] = np.concatenate([[gamma_constants[0]]] + [ncolumns*[g] for g in gamma_constants])\\\n",
    "        .reshape((-1,1))\n",
    "\n",
    "# 5. run the logistic regression on the four datasets\n",
    "def logistic_regression_on_jet(jet):\n",
    "    y = ys[jet]\n",
    "    tx = txs[jet]\n",
    "    \n",
    "    initial_w = weigths[jet] #np.zeros((tx.shape[1], 1)) #\n",
    "    max_iters = 2000\n",
    "    return logistic_regression(y, tx, initial_w, max_iters, gammas[jet])\n",
    "   \n",
    "print(\"Starting to run the logistic regression:\")\n",
    "weigths = [np.zeros((tx.shape[1])) for tx in txs]*4\n",
    "for jet in range(4):\n",
    "    print(\"-Logistic regression on dataset\", str(jet) + \"/3\")\n",
    "    _, weigths[jet] = logistic_regression_on_jet(jet)\n",
    "\n",
    "# 6. Load the test data \n",
    "print(\"Loading test data...\")\n",
    "y_te_loaded, x_te_loaded, ids_te_loaded = load_csv_data(test_data_path, sub_sample=False)\n",
    "print(\"Test data loaded\")\n",
    "\n",
    "# 6. Clean and append the log (in the same exact way we did with the train set)\n",
    "x_te, ids_te = clean_input_data(x_te_loaded.copy(), ids_te_loaded.copy(), corr=1, dimension_expansion=5, bool_col=True)\n",
    "\n",
    "x_te, _, _ = concatenate_log(x_te.copy(), mean_log=mean_log, std_log=std_log)\n",
    "print(\"Test data cleaned.\")\n",
    "\n",
    "# 7. Build the polynomials\n",
    "tx_te = []\n",
    "for jet in range(4):\n",
    "    tx_te.append(build_poly(x_te[jet], degree))\n",
    "print(\"The test polynomials have been built\")\n",
    "\n",
    "# 8. Predict and concatenate the predicitions\n",
    "y_te_pred = []\n",
    "for jet in range(4):\n",
    "    y_te_pred.append(predict_labels(weigths[jet], tx_te[jet]))\n",
    "    \n",
    "for jet in range(4):\n",
    "    ids_te[jet] = ids_te[jet].reshape((-1, 1))\n",
    "y_pred = np.row_stack([y_te_pred[0], y_te_pred[1], y_te_pred[2], y_te_pred[3]])\n",
    "ids = np.row_stack([ids_te[0], ids_te[1], ids_te[2], ids_te[3]])\n",
    "\n",
    "print(\"I predicted \", str((y_pred==-1).sum()), \"-1s and \", str((y_pred==1).sum()), \"1s\")\n",
    "\n",
    "# 9. Store the predictions \n",
    "sub_file_name = \"predictions\"\n",
    "create_csv_submission(ids, y_pred, sub_file_name)\n",
    "print(\"Prediction store in file\" + sub_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
