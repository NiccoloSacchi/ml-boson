{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from cleaner import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explain how and why we cleaned the data. Moreover, we pre-compute some values that will be either hardcoded or stored to file so that the cleaning prosess will be the same both for the train and for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and merge training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../dataset/train.csv\"\n",
    "y_loaded, data_loaded, _ = load_csv_data(data_path)\n",
    "data_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../dataset/test.csv\"\n",
    "_, data_test_loaded, _ = load_csv_data(data_path)\n",
    "data_test_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = np.concatenate((data_loaded,data_test_loaded), axis=0)\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ditributions \n",
    "The first step to understand the data is to plot it. Here we plot (1) every value of every feature, (2) the histogram and (3) the distribution for each feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot all values\n",
    "Plotting all the values can be usefull to determine if and how many outliers there are and also identify the categorical features. In these plots we consider both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(data_merged, col_labels = column_labels(), title=\"values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are very few outliers and that, given the amount of data, they shouldn't affect much the computation of the mean and of the standard deviation. Moreover, the only categorical feature is PRI_jet_num (column 22), we will study further what this feature represents and if/how it affect other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the histograms and the distributions\n",
    "In the graps below we plotted the histogram (left column) and the distribution (rigth column) of each feature depending on the output. In this way, we can compare the distribution of each feature and check by eye if it changes depending on the output y. In these graphs we can consider only the training data since we need the value of the output y to split the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(data_loaded, y_loaded, col_labels = column_labels(), title = \"\", normed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are the distribution of the four features PRI_tau_phi, PRI_lep_pt, PRI_lep_phi, PRI_met_phi (respectively columns 15, 16, 18, 20) seems to be independent from the output y. We will try to drop these columns and verify how it affects out model. Finally, we notice the precence of lot of -999 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our approach: split data depending on the feature PRI_jet_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why divide data depending on jet numbers and which columns can we drop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading information on the dataset we learned that the jet number affects the presence of -999 (invalid) values in other features. Therefore, we splitted our input data in four sets depending on the jet number and verified it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data depending on the jet number (column 22) that is a categorical number in {0, 1, 2, 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data_merged.copy()\n",
    "jets_0 = all_data[all_data[:, 22]==0, :]\n",
    "jets_1 = all_data[all_data[:, 22]==1, :]\n",
    "jets_2 = all_data[all_data[:, 22]==2, :]\n",
    "jets_3 = all_data[all_data[:, 22]==3, :]\n",
    "jets_0.shape, jets_1.shape, jets_2.shape, jets_3.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the -999 values?\n",
    "- jet = 0: columns [4, 5, 6, 12, 23, 24, 25, 26, 27, 28] contain only -999 values, 26.1% of entries in the first column have -999\n",
    "- jet = 1: columns [4, 5, 6, 12, 26, 27, 28] contain only -999 values, 7.6% of entries in the first column have -999\n",
    "- jet = 2: 3% of entries in the first column have -999\n",
    "- jet = 3: 1.4% of entries in the first column have -999\n",
    "\n",
    "We decided to drop in every set the columns that store only -999 values (since they do not keep any information). However, the first feature (DER_mass_MMC) is the only one that contains -999 values in all the 4 sets without filling completely the column. We tried to impute this invalid value with mean, std, median and also simply subtituted it with 0s (after standardization). After several trials we understood that this invalid value didn't \"fit\" in the column and therefore it could need a different weight from the other values of the same column. Therefore, we opted for the more versatile solution: add a boolean column to indicate the position of the -999 values and delete them from first column. By \"delete\" we mean: substitute them with 0s after standardization, where during standardization we ignored those invalid values so that they didn't affect mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet, cur_set in enumerate([jets_0, jets_1, jets_2, jets_3]):\n",
    "    print(\"Features in the dataset with jet=\", jet, \"contains this many values != -999\")\n",
    "    for col in range(30):\n",
    "        print(col, np.sum(cur_set[:, col] != -999))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the 0 values?\n",
    "- jet = 0: column 29 contains only 0s and, obviously, column 22 too since it stores the jet num.\n",
    "- jet = 1: spread\n",
    "- jet = 2: spread\n",
    "- jet = 3: spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet, cur_set in enumerate([jets_0, jets_1, jets_2, jets_3]):\n",
    "    print(\"Features in the dataset with jet=\", jet, \"contains this many values != 0\")\n",
    "    for col in range(30):\n",
    "        print(col, np.sum(cur_set[:, col] != 0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this first analysis we surely want to drop the following columns since they do not contain any useful information:\n",
    "- jet = 0: [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "- jet = 1: [4, 5, 6, 12, 22, 26, 27, 28] \n",
    "- jet = 2: [22]\n",
    "- jet = 3: [22]\n",
    "\n",
    "The column 22 is dropped in every obtained dataset since it stores, within each dataset, a constant representing the jet number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the correlation between the features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deciding to split the data and train the sets independenlty we study if each dataset could be simplified further. In this step we verify if there are some highly correlated features within each one of the four datasets. If so, it is worth trying to drop all but 1 columns in a set of correlated features and check how the model is influenced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the data and drop the useless columns as explained in the previous point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "datasets, _ = split_input_data(data_merged.copy()) # split and drop\n",
    "datasets[0].shape, datasets[1].shape, datasets[2].shape, datasets[3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the correlation matrix for each one of the 4 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation matrices\n",
    "corr_matrices = [None]*4\n",
    "for jet in range(4):\n",
    "    # don't consider the first column since it contains nan values (we will simply keep that column)\n",
    "    corr_matrices[jet] = np.corrcoef(datasets[jet][:, 1:].T) \n",
    "    \n",
    "    # to keep the same indexing of the columns just add one row above and one column at the left\n",
    "    corr_matrices[jet] = np.column_stack((np.zeros((corr_matrices[jet].shape[0], 1)), corr_matrices[jet]))   \n",
    "    corr_matrices[jet] = np.row_stack((np.zeros((1, corr_matrices[jet].shape[1])), corr_matrices[jet]))\n",
    "\n",
    "corr_matrices[0].shape, corr_matrices[1].shape, corr_matrices[2].shape, corr_matrices[3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For some correlation values check which are the features that have an higher |correlation|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mapping of |correlations| > min_corr \n",
    "min_correlations = [0.7 ,  0.75,  0.8 ,  0.85,  0.9 ,  0.95]\n",
    "corr_mappings = {} # mapping: jet -> minimum correlation -> features -> list of correlated features\n",
    "for jet in range(4): # for each dataset build the correlation mapping\n",
    "    corr_mappings[jet] = {}\n",
    "    for min_corr in min_correlations: #for each min correlation considered\n",
    "        corr_mappings[jet][min_corr] = {}\n",
    "        corr_matrix_bool = np.abs(corr_matrices[jet]) > min_corr \n",
    "        nfeature = corr_matrix_bool.shape[0]\n",
    "        # i is surely correlated to itself, drop that (useless) information\n",
    "        for i in range(nfeature):\n",
    "            corr_matrix_bool[i][i] = False\n",
    "\n",
    "        # compute the mapping of correlations\n",
    "        for i in range(nfeature):\n",
    "            c = np.where(corr_matrix_bool[i])[0].tolist()\n",
    "            if len(c) > 0: # if it is not correlated to any other column then ignore it\n",
    "                corr_mappings[jet][min_corr][i] = c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, compute the columns that could be dropped for each considered minimum correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty(map_):\n",
    "    for k in map_:\n",
    "        if len(map_[k]) > 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# jet -> minimum correlation -> features -> list of correlated features\n",
    "\n",
    "tobe_deleted = {} # mapping: jet -> minimum correlation -> list of columns that can be dropped\n",
    "for jet in range(4): # for each dataset build the correlation mapping\n",
    "    tobe_deleted[jet] = {}\n",
    "    for min_corr, corr in corr_mappings[jet].items():\n",
    "        tobe_deleted[jet][min_corr] = []\n",
    "        # fetch all the columns that can be deleted and put them in tobe_deleted\n",
    "        while not empty(corr): \n",
    "            longer_key = -1 \n",
    "            longer_length = 0\n",
    "\n",
    "            # look for the longer list\n",
    "            for key in corr:\n",
    "                curr_length = len(corr[key])\n",
    "                if curr_length > longer_length:\n",
    "                    longer_length = curr_length\n",
    "                    longer_key = key\n",
    "\n",
    "            tobe_deleted[jet][min_corr].append(corr[longer_key])\n",
    "            # delete all the columns that are correlated to column longer_key\n",
    "            # i.e. all the column whose index is in  corr[longer_key]\n",
    "            for corr_colum in corr[longer_key]:\n",
    "                corr[corr_colum] = []\n",
    "\n",
    "            # since those columns have been dropped they must be removed from all the other lists\n",
    "            for key in corr: \n",
    "                if key != longer_key:\n",
    "                    corr[key] = list(set(corr[key]) - set(corr[longer_key]))\n",
    "            corr[longer_key] = []\n",
    "\n",
    "        tobe_deleted[jet][min_corr] = [val for sublist in tobe_deleted[jet][min_corr] for val in sublist]\n",
    "        tobe_deleted[jet][min_corr].sort()\n",
    "tobe_deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The found maps show, for evey dataset and for every chosen minimum correlation a list of features that can be dropped, e.g. tobe_deleted[2][0.85] contains a list of features in the jet=2 dataset which have a |correlation| > 0.85 with at least one of the feature that is kept in the dataset (and therefore can be dropped)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must use the same standardisation process both for preparing the training set and the test set. Therefore, we precompute the mean and the standard deviation that will be used for during standardization. Since the -999 values will be dropped from the first column we remove them before computing the mean and the std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fill_with_nan_list(data_merged.copy(), nan_values=[0, -999])\n",
    "means = np.nanmean(data, axis=0)\n",
    "std_devs = np.nanstd(data, axis=0)\n",
    "means, std_devs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hardcode these values for the standardization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentiles computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A techinque we tried and that allowed to increase the ratio of correctly predicted ouputs is what we called dimension expansion. We divide each feature in N feature using the percentiles, e.g. if N=2 we take the median and split a column in 2 new columns, one with all the entries below the median and one with all the entries above the median. This allows to split the dimension of the input in deifferent intervals in which the regression can be trained independently (immagine if you have to fit an exponential with a 1-degree polynomial, you split the input in 2 and use two 1-degree polynomials instead of 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, separate the data in the four datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the merged dataset\n",
    "x_all, _ = clean_input_data(data_merged.copy())\n",
    "data_merged = fill_with_nan_list(data_merged, nan_values=[-999])\n",
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, for each dataset, for each feature, computer the percentiles that will be used to separate that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]\n",
    "# gerarchical maps: jet -> column -> percentile\n",
    "percentiles = {} \n",
    "for jet, d in enumerate(x_all):\n",
    "    percentiles[jet] = {} \n",
    "    \n",
    "    scan_perc = list(range(0, 101, 5))\n",
    "    #col_perc = np.zeros((len(scan_perc), data_merged.shape[1]))\n",
    "    for col in range(d.shape[1]): # scan columns\n",
    "        percentiles[jet][col] = {}\n",
    "        for p in scan_perc:\n",
    "             percentiles[jet][col][p] = np.nanpercentile(d[:, col], p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, store to file the computed percentiles so that they will be used both on the train and on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the computed percentiles \n",
    "file_path = \"percentiles.json\"\n",
    "json.dump(percentiles, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the model using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Load the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../dataset/train.csv\"\n",
    "y_loaded, x_loaded, ids_te = load_csv_data(data_path, sub_sample=False)\n",
    "y_loaded = y_loaded.reshape((-1, 1))\n",
    "y_loaded.shape, x_loaded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Clean the dataset (also choosing parameters for the cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = 1\n",
    "dimension_expansion = 2\n",
    "bool_col = True\n",
    "xs, ys = clean_input_data(x_loaded.copy(), y_loaded.copy(), corr=corr, \n",
    "                                dimension_expansion=dimension_expansion, bool_col=bool_col)\n",
    "for i in range(len(ys)):\n",
    "    ys[i][ys[i]== -1] = 0\n",
    "xs[0].shape, xs[1].shape, xs[2].shape, xs[3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Build the polynomial on each one of the 4 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the degree and build a polynomial out of each onw of the 4 datasets\n",
    "degree = 3\n",
    "txs = [None]*4\n",
    "for jet in range(4):\n",
    "    txs[jet], _ = build_poly_train(xs[jet], degree)\n",
    "    print(txs[jet].shape, ys[jet].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Set the initial gamma \n",
    "We noticed that the higher the degree the lower was the gamma needed to have a stable gradient descent leading to a higher coverging time. Therefore, instead of using a scalar gamma we used an array of gammas and tuned them independently. In particular, this array of gammas is as long as the gradient of L(w) and allows for choosing a different step size for different parts of the gradient therefore achieving more versatility.\n",
    "Another solution would have been to normalize after building the polynomial (still normalizing the test polynomial and the train polynomial with the same constants), but we achieved a higher converging time with the latter therefore we opted for the array of gammas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma_constants is an array of tuned constants we use to build our vectorial tgamma \n",
    "gamma_constants = [1e-5, 1e-6, 1e-7, 1e-10, 1e-12, 1e-15] # only up to the 6th degree since we didn't notices any improvements\n",
    "gammas = [None]*4\n",
    "print(\"The gamma should be as long as the gradient vector (whose length is equal to the number of columns in tx)\")\n",
    "for jet in range(4):\n",
    "    ncolumns = xs[jet].shape[1] # I build the gamma depending on the columns of the dataset 'jet' and on the degree \n",
    "    gammas[jet] = np.concatenate([[gamma_constants[0]]] + [ncolumns*[g] for g in gamma_constants[:degree]])\\\n",
    "        .reshape((-1,1))\n",
    "    print(txs[jet].shape, gammas[jet].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_on_jet(jet):\n",
    "    y = ys[jet]\n",
    "    tx = txs[jet]\n",
    "    \n",
    "    initial_w = np.zeros((tx.shape[1], 1))\n",
    "    max_iters = 500\n",
    "    return logistic_regression(y, tx, initial_w, max_iters, gammas[jet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will have four weigth vectors, one per dataset\n",
    "weigths = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First dataset (jet = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet = 0\n",
    "loss, w = logistic_regression_on_jet(jet)\n",
    "weigths.append(w)\n",
    "print(\"Final prediction ratio: \", compute_loss(ys[jet], txs[jet], w, costfunc=CostFunction.SUCCESS_RATIO))\n",
    "#0.844297345204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second dataset (jet = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet = 1\n",
    "loss, w = logistic_regression_on_jet(jet)\n",
    "weigths.append(w)\n",
    "print(\"Final prediction ratio: \", compute_loss(ys[jet], txs[jet], w, costfunc=CostFunction.SUCCESS_RATIO))\n",
    "#0.803349010285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Third dataset (jet = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet = 2\n",
    "loss, w = logistic_regression_on_jet(jet)\n",
    "weigths.append(w)\n",
    "print(\"Final prediction ratio: \", compute_loss(ys[jet], txs[jet], w, costfunc=CostFunction.SUCCESS_RATIO))\n",
    "#0.828614217252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fourth dataset (jet = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet = 3\n",
    "loss, w = logistic_regression_on_jet(jet)\n",
    "weigths.append(w)\n",
    "print(\"Final prediction ratio: \", compute_loss(ys[jet], txs[jet], w, costfunc=CostFunction.SUCCESS_RATIO))\n",
    "#0.824365854777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create submit file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../dataset/test.csv\"\n",
    "y_te_loaded, x_te_loaded, ids_te_loaded = load_csv_data(data_path, sub_sample=False)\n",
    "y_te_loaded.shape, x_te_loaded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Clean\n",
    "We clean the test dataset in the exact same way we cleaned the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te, ids_te = clean_input_data(x_te_loaded.copy(), ids_te_loaded.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Build the polynomial\n",
    "We build the polynomials (one per dataset) with the same degree we used with the train the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_te = []\n",
    "for x_te_ in x_te:\n",
    "    tx_te.append(build_poly(x_te_, degree))\n",
    "tx_te[0].shape, tx_te[1].shape, tx_te[2].shape, tx_te[3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  d. Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te_pred = []\n",
    "for jet in range(4):\n",
    "    y_te_pred.append(predict_labels(weigths[jet], tx_te[jet]))\n",
    "y_te_pred[0].shape, y_te_pred[1].shape, y_te_pred[2].shape, y_te_pred[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the results\n",
    "for jet in range(4):\n",
    "    ids_te[jet] = ids_te[jet].reshape((-1, 1))\n",
    "y_pred = np.row_stack([y_te_pred[0], y_te_pred[1], y_te_pred[2], y_te_pred[3]])\n",
    "ids = np.row_stack([ids_te[0], ids_te[1], ids_te[2], ids_te[3]])\n",
    "y_pred.shape, ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it makes sense\n",
    "(y_pred==-1).sum(), (y_pred==1).sum()\n",
    "# (393191, 175047)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Store predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predictions\n",
    "sub_file_name = \"predictions\"\n",
    "create_csv_submission(ids, y_pred, sub_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store/load weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jet in range(4):\n",
    "    #file_path = \"weigths/no_perc/w1\"\n",
    "    file_path = \"../weigths/\" + \"w\" + str(jet)\n",
    "    json.dump(weigths[jet].tolist(), codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigths = []\n",
    "for weigth_name in [\"w0\", \"w1\", \"w2\", \"w3\"]:\n",
    "    file_path = \"weigths/with_perc3/\"+weigth_name\n",
    "    obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "    weigths.append(np.array(json.loads(obj_text)))\n",
    "weigths[0].shape, weigths[1].shape, weigths[2].shape, weigths[3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
