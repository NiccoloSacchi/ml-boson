{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, check the paths to the train file and to the test file are correct:\n",
      "../../dataset/train.csv\n",
      "../../dataset/test.csv\n",
      "Loading train data...\n",
      "Train data loaded\n",
      "Capping the outliers\n",
      "Jet 0 useless columns dropped: [4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "Jet 1 useless columns dropped: [4, 5, 6, 12, 22, 26, 27, 28]\n",
      "Jet 2 useless columns dropped: [22]\n",
      "Jet 3 useless columns dropped: [22]\n",
      "Train data cleaned\n",
      "The train polynomials have been built\n",
      "Starting to run the logistic regression:\n",
      "-Logistic regression on dataset 0/3\n",
      "Gradient Descent(0/1999): loss=64268.2203846304, prediction ratio=0.7449280874360694\n",
      "Gradient Descent(50/1999): loss=35844.15651465327, prediction ratio=0.8429033258935273\n",
      "Gradient Descent(100/1999): loss=35324.298035468426, prediction ratio=0.8453054157116692\n",
      "Gradient Descent(150/1999): loss=35109.24124066348, prediction ratio=0.8456256943540881\n",
      "Gradient Descent(200/1999): loss=34984.4930347388, prediction ratio=0.84616616456317\n",
      "Gradient Descent(250/1999): loss=34898.75450065154, prediction ratio=0.8465164693283156\n",
      "Gradient Descent(300/1999): loss=34834.50430428537, prediction ratio=0.8466465825267984\n",
      "Gradient Descent(350/1999): loss=34783.95411017786, prediction ratio=0.847116991782851\n",
      "Gradient Descent(400/1999): loss=34742.903799113294, prediction ratio=0.8473672094722409\n",
      "Gradient Descent(450/1999): loss=34708.78232010882, prediction ratio=0.847387226887392\n",
      "Gradient Descent(500/1999): loss=34679.89399689803, prediction ratio=0.8473772181798165\n",
      "Gradient Descent(550/1999): loss=34655.06540378263, prediction ratio=0.8473672094722409\n",
      "Gradient Descent(600/1999): loss=34633.45446978409, prediction ratio=0.8473171659343629\n",
      "Gradient Descent(650/1999): loss=34614.43894677448, prediction ratio=0.8472771311040606\n",
      "Gradient Descent(700/1999): loss=34597.547889978654, prediction ratio=0.8473672094722409\n",
      "Gradient Descent(750/1999): loss=34582.41777548761, prediction ratio=0.8473572007646653\n",
      "Gradient Descent(800/1999): loss=34568.76335138099, prediction ratio=0.84743727042527\n",
      "Gradient Descent(850/1999): loss=34556.35765321322, prediction ratio=0.8474873139631479\n",
      "Gradient Descent(900/1999): loss=34545.01792629409, prediction ratio=0.8474873139631479\n",
      "Gradient Descent(950/1999): loss=34534.595476447386, prediction ratio=0.8474773052555723\n",
      "Gradient Descent(1000/1999): loss=34524.96820581515, prediction ratio=0.8475173400858748\n",
      "Gradient Descent(1050/1999): loss=34516.03502794889, prediction ratio=0.8475673836237526\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from proj1_helpers import load_csv_data, create_csv_submission, predict_labels\n",
    "import numpy as np\n",
    "from cleaner import clean_input_data, concatenate_log\n",
    "from implementations import build_poly, logistic_regression, compute_loss\n",
    "\n",
    "# (CHECK THE PATHs ARE CORRECT)\n",
    "train_data_path = \"../../dataset/train.csv\"\n",
    "test_data_path = \"../../dataset/test.csv\"\n",
    "print(\"Please, check the paths to the train file and to the test file are correct:\")\n",
    "print(train_data_path)\n",
    "print(test_data_path)\n",
    "\n",
    "# 1. load the train data \n",
    "print(\"Loading train data...\")\n",
    "y_loaded, x_loaded, ids_te = load_csv_data(train_data_path, sub_sample=False)\n",
    "y_loaded = y_loaded.reshape((-1, 1))\n",
    "print(\"Train data loaded\")\n",
    "\n",
    "# 2. clean the train data and concatenate to each dataset its log\n",
    "xs, ys = clean_input_data(x_loaded.copy(), y_loaded.copy(), corr=1, dimension_expansion=5, bool_col=True)\n",
    "for jet in range(4): # set -1 to 0 \n",
    "    ys[jet][ys[jet]== -1] = 0 \n",
    "    \n",
    "xs, mean_log, std_log = concatenate_log(xs.copy())\n",
    "print(\"Train data cleaned\")\n",
    "\n",
    "# 3. Build the polynomials (one for each one of the 4 datasets)\n",
    "degree = 2\n",
    "txs = [None]*4\n",
    "for jet in range(4):\n",
    "    txs[jet] = build_poly(xs[jet], degree)\n",
    "print(\"The train polynomials have been built\")\n",
    "    \n",
    "# 4. Set the array of gammas for the logistic regression\n",
    "gamma_constants = [1e-5, 1e-6] # one ofr the degree 1 and one for the degree 2\n",
    "gammas = [None]*4\n",
    "for jet in range(4):\n",
    "    ncolumns = xs[jet].shape[1]\n",
    "    gammas[jet] = np.concatenate([[gamma_constants[0]]] + [ncolumns*[g] for g in gamma_constants])\\\n",
    "        .reshape((-1,1))\n",
    "\n",
    "# 5. run the logistic regression on the four datasets\n",
    "def logistic_regression_on_jet(jet):\n",
    "    y = ys[jet]\n",
    "    tx = txs[jet]\n",
    "    \n",
    "    initial_w = weigths[jet] #np.zeros((tx.shape[1], 1)) #\n",
    "    max_iters = 2000\n",
    "    return logistic_regression(y, tx, initial_w, max_iters, gammas[jet])\n",
    "   \n",
    "print(\"Starting to run the logistic regression:\")\n",
    "weigths = [np.zeros((tx.shape[1])) for tx in txs]*4\n",
    "for jet in range(4):\n",
    "    print(\"-Logistic regression on dataset\", str(jet) + \"/3\")\n",
    "    _, weigths[jet] = logistic_regression_on_jet(jet)\n",
    "\n",
    "# 6. Load the test data \n",
    "print(\"Loading test data...\")\n",
    "y_te_loaded, x_te_loaded, ids_te_loaded = load_csv_data(test_data_path, sub_sample=False)\n",
    "print(\"Test data loaded\")\n",
    "\n",
    "# 6. Clean and append the log (in the same exact way we did with the train set)\n",
    "x_te, ids_te = clean_input_data(x_te_loaded.copy(), ids_te_loaded.copy(), corr=1, dimension_expansion=5, bool_col=True)\n",
    "\n",
    "x_te, _, _ = concatenate_log(x_te.copy(), mean_log=mean_log, std_log=std_log)\n",
    "print(\"Test data cleaned.\")\n",
    "\n",
    "# 7. Build the polynomials\n",
    "tx_te = []\n",
    "for jet in range(4):\n",
    "    tx_te.append(build_poly(x_te[jet], degree))\n",
    "print(\"The test polynomials have been built\")\n",
    "\n",
    "# 8. Predict and concatenate the predicitions\n",
    "y_te_pred = []\n",
    "for jet in range(4):\n",
    "    y_te_pred.append(predict_labels(weigths[jet], tx_te[jet]))\n",
    "    \n",
    "for jet in range(4):\n",
    "    ids_te[jet] = ids_te[jet].reshape((-1, 1))\n",
    "y_pred = np.row_stack([y_te_pred[0], y_te_pred[1], y_te_pred[2], y_te_pred[3]])\n",
    "ids = np.row_stack([ids_te[0], ids_te[1], ids_te[2], ids_te[3]])\n",
    "\n",
    "print(\"I predicted \", str((y_pred==-1).sum()), \"-1s and \", str((y_pred==1).sum()), \"1s\")\n",
    "\n",
    "# 9. Store the predictions \n",
    "sub_file_name = \"predictions\"\n",
    "create_csv_submission(ids, y_pred, sub_file_name)\n",
    "print(\"Prediction store in file\" + sub_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
